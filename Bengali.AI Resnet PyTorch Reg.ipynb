{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import time\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/Lexie88rus/Bengali_AI_Competition/raw/master/assets/samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengali.AI Resnet CutMix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the path to data and load the csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# setup the input data folder\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "# load the dataframes with labels\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_labels = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "class_map = pd.read_csv(DATA_PATH + 'class_map.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"BengaliAI\"\n",
    "EXPERIMENT_NAME = \"Resnet18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VERTA_EMAIL'] = 'astakhova.aleksandra@gmail.com'\n",
    "os.environ['VERTA_DEV_KEY'] = 'd7ee32b5-bbd0-4c4c-a2ec-a070848021be'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "set existing Project: BengaliAI\n",
      "created new Experiment: Resnet18\n",
      "created new ExperimentRun: Run 3242015807969897496865\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "run.log_tag('Resnet18')\n",
    "run.log_tag('CutMix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test and train sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    '''\n",
    "    Helper function to load all train and test images\n",
    "    '''\n",
    "    train_list = []\n",
    "    for i in range(0,4):\n",
    "        train_list.append(pd.read_parquet(DATA_PATH + 'train_image_data_{}.parquet'.format(i)))\n",
    "    train = pd.concat(train_list, ignore_index=True)\n",
    "    \n",
    "    test_list = []\n",
    "    for i in range(0,4):\n",
    "        test_list.append(pd.read_parquet(DATA_PATH + 'test_image_data_{}.parquet'.format(i)))\n",
    "    test = pd.concat(test_list, ignore_index=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and data augmentation are exteremely important for the training of deep learning models. I use the adaptive thresholding to binarize the input images and a simple data augmentation pipeline consisting of random crop-resize and slight rotation of the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup image hight and width\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "\n",
    "SIZE = 32\n",
    "\n",
    "def threshold_image(img):\n",
    "    '''\n",
    "    Helper function for thresholding the images\n",
    "    '''\n",
    "    gray = PIL.Image.fromarray(np.uint8(img), 'L')\n",
    "    ret,th = cv.threshold(np.array(gray),0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    return th\n",
    "\n",
    "def train_transforms(p=.5):\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.CenterCrop(height = 128, width = 128),\n",
    "        albu.Rotate(limit=3, p=p),\n",
    "        albu.Resize(height = SIZE, width = SIZE)\n",
    "    ], p=1.0)\n",
    "\n",
    "def valid_transforms():\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.CenterCrop(height = 128, width = 128),\n",
    "        albu.Resize(height = SIZE, width = SIZE)\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a custom pytorch dataset, which will produce images and corresponding labels out of the traing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions to retrieve the images from the dataset in training and validation modes\n",
    "'''\n",
    "\n",
    "def get_image(idx, df, labels):\n",
    "    '''\n",
    "    Helper function to get the image and label from the training set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n",
    "    # get the labels\n",
    "    row = labels[labels.image_id == image_id]\n",
    "    \n",
    "    # return labels as tuple\n",
    "    labels = row['grapheme_root'].values[0], \\\n",
    "    row['vowel_diacritic'].values[0], \\\n",
    "    row['consonant_diacritic'].values[0]\n",
    "    \n",
    "    return img, labels\n",
    "\n",
    "def get_validation(idx, df):\n",
    "    '''\n",
    "    Helper function to get the validation image and image_id from the test set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n",
    "    return img, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDataset(Dataset):\n",
    "    '''\n",
    "    Create a custom Bengali images dataset\n",
    "    '''\n",
    "    def __init__(self, df_images, transforms, df_labels = None, validation = False):\n",
    "        '''\n",
    "        Init function\n",
    "        INPUT:\n",
    "            df_images - dataframe with the images\n",
    "            transforms - data transforms\n",
    "            df_labels - datafrane containing the target labels\n",
    "            validation - flag indication if the dataset is for training or for validation\n",
    "        '''\n",
    "        self.df_images = df_images\n",
    "        self.df_labels = df_labels\n",
    "        self.transforms = transforms\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.validation:\n",
    "            # get the image\n",
    "            img, label = get_image(idx, self.df_images, self.df_labels)\n",
    "            # threshold the image\n",
    "            img = threshold_image(img)\n",
    "            # transform the image\n",
    "            #img = img.astype(np.uint8)\n",
    "            aug = self.transforms(image = img)\n",
    "            img = TF.to_tensor(aug['image'])\n",
    "            img = np.tile(img, (3,1,1))\n",
    "            return img, label\n",
    "        else:\n",
    "            # get the image\n",
    "            img, image_id = get_validation(idx, self.df_images)\n",
    "            # threshold the image\n",
    "            img = threshold_image(img)\n",
    "            # transform the image\n",
    "            #img = img.astype(np.uint8)\n",
    "            aug = self.transforms(image = img)\n",
    "            # return transformed image and corresponding image_id (instead of label) to create submission\n",
    "            img = TF.to_tensor(aug['image'])\n",
    "            img = np.tile(img, (3,1,1))\n",
    "            return img, image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that everything is correct. Let's try to retrieve couple of images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "train_dataset = BengaliDataset(train, train_transforms(), train_labels)\n",
    "# create a sample trainloader\n",
    "sample_trainloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample train data\n",
    "for img, labels in sample_trainloader:\n",
    "    \n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i][0].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('./kalpurush.ttf')\n",
    "        grapheme_root = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[i].set_title('{}, {}, {}'.format(grapheme_root, vowel_diacritic, consonant_diacritic), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * Bottleneck.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * Bottleneck.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * Bottleneck.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * Bottleneck.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, dataset, depth, num_classes, bottleneck=False):\n",
    "        super(ResNet, self).__init__()        \n",
    "        self.dataset = dataset\n",
    "        if self.dataset.startswith('cifar'):\n",
    "            self.inplanes = 16\n",
    "            print(bottleneck)\n",
    "            if bottleneck == True:\n",
    "                n = int((depth - 2) / 9)\n",
    "                block = Bottleneck\n",
    "            else:\n",
    "                n = int((depth - 2) / 6)\n",
    "                block = BasicBlock\n",
    "\n",
    "            self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.layer1 = self._make_layer(block, 16, n)\n",
    "            self.layer2 = self._make_layer(block, 32, n, stride=2)\n",
    "            self.layer3 = self._make_layer(block, 64, n, stride=2) \n",
    "            self.avgpool = nn.AvgPool2d(8)\n",
    "            self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        elif dataset == 'imagenet':\n",
    "            blocks ={18: BasicBlock, 34: BasicBlock, 50: Bottleneck, 101: Bottleneck, 152: Bottleneck, 200: Bottleneck}\n",
    "            layers ={18: [2, 2, 2, 2], 34: [3, 4, 6, 3], 50: [3, 4, 6, 3], 101: [3, 4, 23, 3], 152: [3, 8, 36, 3], 200: [3, 24, 36, 3]}\n",
    "            assert layers[depth], 'invalid detph for ResNet (depth should be one of 18, 34, 50, 101, 152, and 200)'\n",
    "\n",
    "            self.inplanes = 64\n",
    "            self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            self.layer1 = self._make_layer(blocks[depth], 64, layers[depth][0])\n",
    "            self.layer2 = self._make_layer(blocks[depth], 128, layers[depth][1], stride=2)\n",
    "            self.layer3 = self._make_layer(blocks[depth], 256, layers[depth][2], stride=2)\n",
    "            self.layer4 = self._make_layer(blocks[depth], 512, layers[depth][3], stride=2)\n",
    "            self.avgpool = nn.AvgPool2d(7) \n",
    "            self.fc = nn.Linear(512 * blocks[depth].expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.dataset == 'cifar10' or self.dataset == 'cifar100':\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "\n",
    "            x = self.avgpool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        elif self.dataset == 'imagenet':\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "\n",
    "            x = self.avgpool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "backbone_model = ResNet('cifar100', depth=18, num_classes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom model for the Bengali images\n",
    "backbone model may be replaced with any other archirtecture :)\n",
    "'''\n",
    "\n",
    "class BengaliModel(nn.Module):\n",
    "    def __init__(self, backbone_model):\n",
    "        super(BengaliModel, self).__init__()\n",
    "        #self.conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)\n",
    "        self.backbone_model = backbone_model\n",
    "        self.fc1 = nn.Linear(in_features=500, out_features=168) # grapheme_root\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=11) # vowel_diacritic\n",
    "        self.fc3 = nn.Linear(in_features=500, out_features=7) # consonant_diacritic\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass through the backbone model\n",
    "        #y = self.conv(x)\n",
    "        y = self.backbone_model(x)\n",
    "        \n",
    "        # multi-output\n",
    "        grapheme_root = self.fc1(y)\n",
    "        vowel_diacritic = self.fc2(y)\n",
    "        consonant_diacritic = self.fc3(y)\n",
    "        \n",
    "        return grapheme_root, vowel_diacritic, consonant_diacritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the final model\n",
    "model = BengaliModel(backbone_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set for the modelling:\n",
    "\n",
    "First, let's start with defining the hyperparameters. In this notebook I won't be actually training the model, that is why the number of epochs is 0. I trained the model on my own machine and will just load the weights here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.2\n",
    "batch_size = 128\n",
    "epochs = 10 # change this value to actually train the model\n",
    "learning_rate = 0.001\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"test_split\", test_split)\n",
    "run.log_hyperparameter(\"batch_size\", batch_size)\n",
    "run.log_hyperparameter(\"epochs\", epochs)\n",
    "run.log_hyperparameter(\"learning_rate\", learning_rate)\n",
    "run.log_hyperparameter(\"image_size\", SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the dataset and samplers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "\n",
    "# split the dataset into test and train\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "testloader = DataLoader(train_dataset, batch_size=32, sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizer, only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"optimizer\", \"Adam\")\n",
    "run.log_hyperparameter(\"loss\", \"CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup training device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the logging. I will write the log into pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy'\n",
    "                                      ,'Test loss', 'Test accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to the training device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# I'm just loading the weights instead of training\n",
    "#state = torch.load('./resnet18_9.pth', map_location=lambda storage, loc: storage)\n",
    "#model.load_state_dict(state[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\lexie\\lib\\site-packages\\ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657d26cc50994879afd822ccd43ea1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_accuracy(ps, labels):\n",
    "    '''\n",
    "    Helper function to calculate the accuracy given the labels and the output of the model\n",
    "    '''\n",
    "    ps = torch.exp(ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy = torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    return accuracy\n",
    "\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    train_accuracy = 0\n",
    "    top3_train_accuracy = 0 \n",
    "    for inputs, labels in tqdm_notebook(trainloader):\n",
    "        steps += 1\n",
    "        # move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + \\\n",
    "        criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # get the average accuracy\n",
    "        train_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "        \n",
    "\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    model.eval()\n",
    "    # run validation on the test set\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "            \n",
    "            grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "            batch_loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "            test_loss += batch_loss.item()\n",
    "            \n",
    "            scheduler.step(test_loss)\n",
    "\n",
    "            # Calculate test top-1 accuracy\n",
    "            test_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "    \n",
    "    # print out the training stats\n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Time per epoch: {time_elapsed:.4f}.. \"\n",
    "          f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n",
    "          f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n",
    "          f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n",
    "          f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n",
    "          f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \")\n",
    "    \n",
    "    filename = 'resnet18_cutmix_'+ str(epoch+1) + '.pth'\n",
    "    checkpoint = {'state_dict': model.state_dict()}\n",
    "    torch.save(checkpoint, filename)\n",
    "    \n",
    "    run.log_observation(\"time_per_epoch\", time_elapsed)\n",
    "    run.log_observation(\"time_per_step\", time_elapsed/len(trainloader))\n",
    "    run.log_observation(\"train_loss\", running_loss/len(trainloader))\n",
    "    run.log_observation(\"test_loss\", test_loss/len(testloader))\n",
    "    run.log_observation(\"train_accuracy\", train_accuracy/len(trainloader))\n",
    "    run.log_observation(\"test_accuracy\", test_accuracy/len(testloader))\n",
    "\n",
    "    # write to the training log\n",
    "    train_stats = train_stats.append({'Epoch': epoch, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader),\n",
    "                                      'Train accuracy': train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader),\n",
    "                                      'Test accuracy': test_accuracy/len(testloader)}, ignore_index=True)\n",
    "\n",
    "    running_loss = 0\n",
    "    steps = 0\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'resnet18_cutmix_'+ str(epochs) + '.pth'\n",
    "\n",
    "checkpoint = {'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.to_csv('resnet18_cutmix_train_stats_{}.csv'.format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('model', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('train_stats', train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(train_stats['Train loss'], label='train')\n",
    "plt.plot(train_stats['Test loss'], label='test')\n",
    "plt.title('Loss over epoch')\n",
    "plt.legend()\n",
    "run.log_image(\"loss\", plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy\n",
    "plt.plot(train_stats['Train accuracy'], label='train')\n",
    "plt.plot(train_stats['Test accuracy'], label='test')\n",
    "plt.title('Accuracy over epoch')\n",
    "plt.legend()\n",
    "run.log_image(\"accuracy\", plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize some sample predictions from the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[137, 236]' is invalid for input of size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-7306b6e522e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHEIGHT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWIDTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFontProperties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[137, 236]' is invalid for input of size 1024"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample train data\n",
    "model.eval()\n",
    "for img, labels in testloader:\n",
    "    img, labels = img.to(device), [label.to(device) for label in labels]\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(img)\n",
    "    \n",
    "    img = img.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # visualize the inputs\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(10,15))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[0].imshow(TF.to_pil_image(img[i].reshape(HEIGHT, WIDTH)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('../input/bengaliaiutils/kalpurush.ttf')\n",
    "        grapheme_root_str = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic_str = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic_str = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[0].set_title('{}, {}, {}'.format(grapheme_root_str, vowel_diacritic_str, consonant_diacritic_str), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "        \n",
    "        # analyze grapheme root prediction\n",
    "        ps_root = F.softmax(grapheme_root[i])\n",
    "        top10_p, top10_class = ps_root.topk(10, dim=0)\n",
    "        \n",
    "        top10_p = top10_p.detach().numpy()\n",
    "        top10_class = top10_class.detach().numpy()\n",
    "        \n",
    "        axs[1].bar(range(len(top10_p)), top10_p)\n",
    "        axs[1].set_xticks(range(len(top10_p)))\n",
    "        axs[1].set_xticklabels(top10_class)\n",
    "        axs[1].set_title('grapheme_root: {}'.format(labels[0][i]))\n",
    "        \n",
    "        # analyze vowel prediction\n",
    "        ps_vowel = F.softmax(vowel_diacritic[i])\n",
    "        top11_p, top11_class = ps_vowel.topk(11, dim=0)\n",
    "        \n",
    "        top11_p = top11_p.detach().numpy()\n",
    "        top11_class = top11_class.detach().numpy()\n",
    "        \n",
    "        axs[2].bar(range(len(top11_p)), top11_p)\n",
    "        axs[2].set_xticks(range(len(top11_p)))\n",
    "        axs[2].set_xticklabels(top11_class)\n",
    "        axs[2].set_title('vowel_diacritic: {}'.format(labels[1][i]))\n",
    "        \n",
    "        # analyze consonant prediction\n",
    "        ps_cons = F.softmax(consonant_diacritic[i])\n",
    "        top7_p, top7_class = ps_cons.topk(7, dim=0)\n",
    "        \n",
    "        top7_p = top7_p.detach().numpy()\n",
    "        top7_class = top7_class.detach().numpy()\n",
    "        \n",
    "        axs[3].bar(range(len(top7_p)), top7_p)\n",
    "        axs[3].set_xticks(range(len(top7_p)))\n",
    "        axs[3].set_xticklabels(top7_class)\n",
    "        axs[3].set_title('consonant_diacritic: {}'.format(labels[2][i]))\n",
    "        \n",
    "        plt.show()\n",
    "        break;\n",
    "        \n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "test_dataset = BengaliDataset(test, valid_transforms(), test_labels, validation = True)\n",
    "sample_validloader = DataLoader(test_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the images from validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "for img, image_ids in sample_validloader:\n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        axs[i].set_title(image_ids[i])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_label(ps):\n",
    "    '''\n",
    "    Helper function to get the predicted label given the probabilities from the model output\n",
    "    '''\n",
    "    ps = F.softmax(ps)[0]\n",
    "    top_p, top_class = ps.topk(1, dim=0)\n",
    "        \n",
    "    top_p = top_p.detach().numpy()\n",
    "    top_class = top_class.detach().numpy()\n",
    "    \n",
    "    return top_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission\n",
    "# initialize the dataframe\n",
    "submission = pd.DataFrame(columns=['row_id', 'target'])\n",
    "\n",
    "for imgs, image_ids in validloader:\n",
    "    img = imgs[0]\n",
    "    image_id = image_ids[0]\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    # forward pass to get the output\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(imgs)\n",
    "    \n",
    "    imgs = imgs.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # get the predicted labels\n",
    "    grapheme_root_label = get_predicted_label(grapheme_root)\n",
    "    vowel_diacritic_label = get_predicted_label(vowel_diacritic)\n",
    "    consonant_diacritic_label = get_predicted_label(consonant_diacritic)\n",
    "    \n",
    "    # add the results to the dataframe\n",
    "    submission = submission.append({'row_id':str(image_id)+'_grapheme_root', 'target':grapheme_root_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_vowel_diacritic', 'target':vowel_diacritic_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_consonant_diacritic', 'target':consonant_diacritic_label}, \n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook I created and trained a sample model. This code can't be used for the actual predcitions for the competition. It requires a lot of optiomization, but you can use it as a sample for learning purposes.\n",
    "\n",
    "## References\n",
    "1. [EfficientNet paper](https://arxiv.org/pdf/1905.11946.pdf)\n",
    "2. [efficientnet-pytorch pacckage](https://pypi.org/project/efficientnet-pytorch/)\n",
    "3. [My EDA notebook for Bengali.AI](https://www.kaggle.com/aleksandradeis/bengali-ai-eda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

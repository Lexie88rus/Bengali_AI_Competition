{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import time\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/Lexie88rus/Bengali_AI_Competition/raw/master/assets/samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengali.AI Resnet CutMix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the path to data and load the csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# setup the input data folder\n",
    "DATA_PATH = './data/'\n",
    "PREPROCESSED_PATH = './preprocessed64/'\n",
    "\n",
    "# load the dataframes with labels\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_labels = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "class_map = pd.read_csv(DATA_PATH + 'class_map.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"BengaliAI\"\n",
    "EXPERIMENT_NAME = \"Resnext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VERTA_EMAIL'] = 'astakhova.aleksandra@gmail.com'\n",
    "os.environ['VERTA_DEV_KEY'] = 'd7ee32b5-bbd0-4c4c-a2ec-a070848021be'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "set existing Project: BengaliAI\n",
      "set existing Experiment: Resnext\n",
      "created new ExperimentRun: Run 26761582788055969791\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "run.log_tag('Resnet50')\n",
    "run.log_tag('CutMix')\n",
    "run.log_tag('added more transforms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and data augmentation are exteremely important for the training of deep learning models. I use the adaptive thresholding to binarize the input images and a simple data augmentation pipeline consisting of random crop-resize and slight rotation of the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup image hight and width\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "\n",
    "SIZE = 64\n",
    "\n",
    "def threshold_image(img):\n",
    "    '''\n",
    "    Helper function for thresholding the images\n",
    "    '''\n",
    "    gray = PIL.Image.fromarray(np.uint8(img), 'L')\n",
    "    ret,th = cv.threshold(np.array(gray),0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    return th\n",
    "\n",
    "def train_transforms(p=.5):\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        albu.RandomSizedCrop(min_max_height=(int(SIZE // 1.1), SIZE), height = SIZE, width = SIZE, p=p),\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.Rotate(limit=3, p=p),\n",
    "    ], p=1.0)\n",
    "\n",
    "def valid_transforms():\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.CenterCrop(height = 128, width = 128),\n",
    "        albu.Resize(height = SIZE, width = SIZE)\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a custom pytorch dataset, which will produce images and corresponding labels out of the traing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions to retrieve the images from the dataset in training and validation modes\n",
    "'''\n",
    "\n",
    "def get_image(idx, labels):\n",
    "    '''\n",
    "    Helper function to get the image and label from the training set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = labels.iloc[idx].image_id\n",
    "    filename = PREPROCESSED_PATH + str(image_id) + '.png' \n",
    "    # get the image by id\n",
    "    img = np.asarray(PIL.Image.open(filename))\n",
    "    # get the labels\n",
    "    row = labels[labels.image_id == image_id]\n",
    "    \n",
    "    # return labels as tuple\n",
    "    labels = row['grapheme_root'].values[0], \\\n",
    "    row['vowel_diacritic'].values[0], \\\n",
    "    row['consonant_diacritic'].values[0]\n",
    "    \n",
    "    return img, labels\n",
    "\n",
    "def get_validation(idx, labels):\n",
    "    '''\n",
    "    Helper function to get the validation image and image_id from the test set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = labels.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    filename = PREPROCESSED_PATH + str(image_id) + '.png' \n",
    "    # get the image by id\n",
    "    img = np.asarray(PIL.Image.open(filename))\n",
    "    return img, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDataset(Dataset):\n",
    "    '''\n",
    "    Create a custom Bengali images dataset\n",
    "    '''\n",
    "    def __init__(self, transforms, df_labels = None, validation = False):\n",
    "        '''\n",
    "        Init function\n",
    "        INPUT:\n",
    "            df_images - dataframe with the images\n",
    "            transforms - data transforms\n",
    "            df_labels - datafrane containing the target labels\n",
    "            validation - flag indication if the dataset is for training or for validation\n",
    "        '''\n",
    "        self.df_labels = df_labels\n",
    "        self.transforms = transforms\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.validation:\n",
    "            # get the image\n",
    "            img, label = get_image(idx, self.df_labels)\n",
    "            # transform the image\n",
    "            aug = self.transforms(image = img)\n",
    "            img = TF.to_tensor(aug['image'])\n",
    "            #img = np.tile(img, (3,1,1))\n",
    "            return img, label\n",
    "        else:\n",
    "            # get the image\n",
    "            img, image_id = get_validation(idx, self.df_labels)\n",
    "            # transform the image\n",
    "            #img = np.tile(img, (3,1,1))\n",
    "            return img, image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that everything is correct. Let's try to retrieve couple of images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "train_dataset = BengaliDataset(train_transforms(), train_labels)\n",
    "# create a sample trainloader\n",
    "sample_trainloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample train data\n",
    "for img, labels in sample_trainloader:\n",
    "    \n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i][0].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('./kalpurush.ttf')\n",
    "        grapheme_root = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[i].set_title('{}, {}, {}'.format(grapheme_root, vowel_diacritic, consonant_diacritic), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "\n",
    "\"\"\" \n",
    "Creates a ResNeXt Model as defined in:\n",
    "Xie, S., Girshick, R., Dollár, P., Tu, Z., & He, K. (2016). \n",
    "Aggregated residual transformations for deep neural networks. \n",
    "arXiv preprint arXiv:1611.05431.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Pau Rodríguez López, ISELAB, CVC-UAB\"\n",
    "__email__ = \"pau.rodri1@gmail.com\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "class ResNeXtBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, cardinality, base_width, widen_factor):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            in_channels: input channel dimensionality\n",
    "            out_channels: output channel dimensionality\n",
    "            stride: conv stride. Replaces pooling layer.\n",
    "            cardinality: num of convolution groups.\n",
    "            base_width: base number of channels in each group.\n",
    "            widen_factor: factor to reduce the input dimensionality before convolution.\n",
    "        \"\"\"\n",
    "        super(ResNeXtBottleneck, self).__init__()\n",
    "        width_ratio = out_channels / (widen_factor * 64.)\n",
    "        D = cardinality * int(base_width * width_ratio)\n",
    "        self.conv_reduce = nn.Conv2d(in_channels, D, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn_reduce = nn.BatchNorm2d(D)\n",
    "        self.conv_conv = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(D)\n",
    "        self.conv_expand = nn.Conv2d(D, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn_expand = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut.add_module('shortcut_conv',\n",
    "                                     nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0,\n",
    "                                               bias=False))\n",
    "            self.shortcut.add_module('shortcut_bn', nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        bottleneck = self.conv_reduce.forward(x)\n",
    "        bottleneck = F.relu(self.bn_reduce.forward(bottleneck), inplace=True)\n",
    "        bottleneck = self.conv_conv.forward(bottleneck)\n",
    "        bottleneck = F.relu(self.bn.forward(bottleneck), inplace=True)\n",
    "        bottleneck = self.conv_expand.forward(bottleneck)\n",
    "        bottleneck = self.bn_expand.forward(bottleneck)\n",
    "        residual = self.shortcut.forward(x)\n",
    "        return F.relu(residual + bottleneck, inplace=True)\n",
    "\n",
    "\n",
    "class CifarResNeXt(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNext optimized for the Cifar dataset, as specified in\n",
    "    https://arxiv.org/pdf/1611.05431.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cardinality, depth, nlabels, base_width, widen_factor=4):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            cardinality: number of convolution groups.\n",
    "            depth: number of layers.\n",
    "            nlabels: number of classes\n",
    "            base_width: base number of channels in each group.\n",
    "            widen_factor: factor to adjust the channel dimensionality\n",
    "        \"\"\"\n",
    "        super(CifarResNeXt, self).__init__()\n",
    "        self.cardinality = cardinality\n",
    "        self.depth = depth\n",
    "        self.block_depth = (self.depth - 2) // 9\n",
    "        self.base_width = base_width\n",
    "        self.widen_factor = widen_factor\n",
    "        self.nlabels = nlabels\n",
    "        self.output_size = 64\n",
    "        self.stages = [64, 64 * self.widen_factor, 128 * self.widen_factor, 256 * self.widen_factor]\n",
    "\n",
    "        self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "        self.bn_1 = nn.BatchNorm2d(64)\n",
    "        self.stage_1 = self.block('stage_1', self.stages[0], self.stages[1], 1)\n",
    "        self.stage_2 = self.block('stage_2', self.stages[1], self.stages[2], 2)\n",
    "        self.stage_3 = self.block('stage_3', self.stages[2], self.stages[3], 2)\n",
    "        self.classifier = nn.Linear(self.stages[3], nlabels)\n",
    "        init.kaiming_normal(self.classifier.weight)\n",
    "\n",
    "        for key in self.state_dict():\n",
    "            if key.split('.')[-1] == 'weight':\n",
    "                if 'conv' in key:\n",
    "                    init.kaiming_normal(self.state_dict()[key], mode='fan_out')\n",
    "                if 'bn' in key:\n",
    "                    self.state_dict()[key][...] = 1\n",
    "            elif key.split('.')[-1] == 'bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "\n",
    "    def block(self, name, in_channels, out_channels, pool_stride=2):\n",
    "        \"\"\" Stack n bottleneck modules where n is inferred from the depth of the network.\n",
    "        Args:\n",
    "            name: string name of the current block.\n",
    "            in_channels: number of input channels\n",
    "            out_channels: number of output channels\n",
    "            pool_stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.\n",
    "        Returns: a Module consisting of n sequential bottlenecks.\n",
    "        \"\"\"\n",
    "        block = nn.Sequential()\n",
    "        for bottleneck in range(self.block_depth):\n",
    "            name_ = '%s_bottleneck_%d' % (name, bottleneck)\n",
    "            if bottleneck == 0:\n",
    "                block.add_module(name_, ResNeXtBottleneck(in_channels, out_channels, pool_stride, self.cardinality,\n",
    "                                                          self.base_width, self.widen_factor))\n",
    "            else:\n",
    "                block.add_module(name_,\n",
    "                                 ResNeXtBottleneck(out_channels, out_channels, 1, self.cardinality, self.base_width,\n",
    "                                                   self.widen_factor))\n",
    "        return block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1_3x3.forward(x)\n",
    "        x = F.relu(self.bn_1.forward(x), inplace=True)\n",
    "        x = self.stage_1.forward(x)\n",
    "        x = self.stage_2.forward(x)\n",
    "        x = self.stage_3.forward(x)\n",
    "        x = F.avg_pool2d(x, 8, 1)\n",
    "        x = x.view(-1, self.stages[3])\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\lexie\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "d:\\pycharmprojects\\lexie\\lib\\site-packages\\ipykernel_launcher.py:98: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "backbone_model = CifarResNeXt(cardinality=8, depth=29, nlabels=500, base_width=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom model for the Bengali images\n",
    "backbone model may be replaced with any other archirtecture :)\n",
    "'''\n",
    "\n",
    "class BengaliModel(nn.Module):\n",
    "    def __init__(self, backbone_model):\n",
    "        super(BengaliModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.backbone_model = backbone_model\n",
    "        self.fc1 = nn.Linear(in_features=500, out_features=168) # grapheme_root\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=11) # vowel_diacritic\n",
    "        self.fc3 = nn.Linear(in_features=500, out_features=7) # consonant_diacritic\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass through the backbone model\n",
    "        y = self.conv(x)\n",
    "        y = self.pool(y)\n",
    "        \n",
    "        y = self.backbone_model(y)\n",
    "        \n",
    "        # multi-output\n",
    "        grapheme_root = self.fc1(y)\n",
    "        vowel_diacritic = self.fc2(y)\n",
    "        consonant_diacritic = self.fc3(y)\n",
    "        \n",
    "        return grapheme_root, vowel_diacritic, consonant_diacritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the final model\n",
    "model = BengaliModel(backbone_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set for the modelling:\n",
    "\n",
    "First, let's start with defining the hyperparameters. In this notebook I won't be actually training the model, that is why the number of epochs is 0. I trained the model on my own machine and will just load the weights here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.2\n",
    "batch_size = 64\n",
    "epochs = 50 # change this value to actually train the model\n",
    "learning_rate = 0.001\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"test_split\", test_split)\n",
    "run.log_hyperparameter(\"batch_size\", batch_size)\n",
    "run.log_hyperparameter(\"epochs\", epochs)\n",
    "run.log_hyperparameter(\"learning_rate\", learning_rate)\n",
    "run.log_hyperparameter(\"image_size\", SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the dataset and samplers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "\n",
    "# split the dataset into test and train\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "testloader = DataLoader(train_dataset, batch_size=32, sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizer, only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"optimizer\", \"Adam\")\n",
    "run.log_hyperparameter(\"loss\", \"CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup training device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the logging. I will write the log into pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy'\n",
    "                                      ,'Test loss', 'Test accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = torch.load('resnet50_cutmix_64_transforms_stopped_50.pth', map_location=lambda storage, loc: storage)\n",
    "#model.load_state_dict(state[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to the training device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, learning_rate, epochs):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // (epochs * 0.5))) * (0.1 ** (epoch // (epochs * 0.75)))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "def get_learning_rate(optimizer):\n",
    "    lr = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr += [param_group['lr']]\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\lexie\\lib\\site-packages\\ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456074359c884acd976f5702ca47fff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_accuracy(ps, labels):\n",
    "    '''\n",
    "    Helper function to calculate the accuracy given the labels and the output of the model\n",
    "    '''\n",
    "    ps = torch.exp(ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy = torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    return accuracy\n",
    "\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    train_accuracy = 0\n",
    "    top3_train_accuracy = 0 \n",
    "    for inputs, labels in tqdm_notebook(trainloader):\n",
    "        steps += 1\n",
    "        # move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + \\\n",
    "        criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # get the average accuracy\n",
    "        train_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch, learning_rate, epochs)\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    model.eval()\n",
    "    # run validation on the test set\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "            \n",
    "            grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "            batch_loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "            test_loss += batch_loss.item()\n",
    "            \n",
    "            scheduler.step(test_loss)\n",
    "\n",
    "            # Calculate test top-1 accuracy\n",
    "            test_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "    \n",
    "    # print out the training stats\n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Time per epoch: {time_elapsed:.4f}.. \"\n",
    "          f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n",
    "          f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n",
    "          f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n",
    "          f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n",
    "          f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \")\n",
    "    \n",
    "    filename = 'resnext_'+ str(epoch+1) + '.pth'\n",
    "    checkpoint = {'state_dict': model.state_dict()}\n",
    "    torch.save(checkpoint, filename)\n",
    "    \n",
    "    run.log_observation(\"time_per_epoch\", time_elapsed)\n",
    "    run.log_observation(\"time_per_step\", time_elapsed/len(trainloader))\n",
    "    run.log_observation(\"train_loss\", running_loss/len(trainloader))\n",
    "    run.log_observation(\"test_loss\", test_loss/len(testloader))\n",
    "    run.log_observation(\"train_accuracy\", train_accuracy/len(trainloader))\n",
    "    run.log_observation(\"test_accuracy\", test_accuracy/len(testloader))\n",
    "    run.log_observation(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # write to the training log\n",
    "    train_stats = train_stats.append({'Epoch': epoch, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader),\n",
    "                                      'Train accuracy': train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader),\n",
    "                                      'Test accuracy': test_accuracy/len(testloader)}, ignore_index=True)\n",
    "\n",
    "    running_loss = 0\n",
    "    steps = 0\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'resnext_'+ str(epochs) + '.pth'\n",
    "\n",
    "checkpoint = {'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.to_csv('resnet50_cutmix_64_trf_train_stats_1_{}.csv'.format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('model', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('train_stats', train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(train_stats['Train loss'], label='train')\n",
    "plt.plot(train_stats['Test loss'], label='test')\n",
    "plt.title('Loss over epoch')\n",
    "plt.legend()\n",
    "run.log_image(\"loss\", plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy\n",
    "plt.plot(train_stats['Train accuracy'], label='train')\n",
    "plt.plot(train_stats['Test accuracy'], label='test')\n",
    "plt.title('Accuracy over epoch')\n",
    "plt.legend()\n",
    "run.log_image(\"accuracy\", plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize some sample predictions from the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "model.eval()\n",
    "for img, labels in testloader:\n",
    "    img, labels = img.to(device), [label.to(device) for label in labels]\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(img)\n",
    "    \n",
    "    img = img.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # visualize the inputs\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(10,15))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[0].imshow(TF.to_pil_image(img[i].reshape(HEIGHT, WIDTH)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('../input/bengaliaiutils/kalpurush.ttf')\n",
    "        grapheme_root_str = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic_str = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic_str = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[0].set_title('{}, {}, {}'.format(grapheme_root_str, vowel_diacritic_str, consonant_diacritic_str), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "        \n",
    "        # analyze grapheme root prediction\n",
    "        ps_root = F.softmax(grapheme_root[i])\n",
    "        top10_p, top10_class = ps_root.topk(10, dim=0)\n",
    "        \n",
    "        top10_p = top10_p.detach().numpy()\n",
    "        top10_class = top10_class.detach().numpy()\n",
    "        \n",
    "        axs[1].bar(range(len(top10_p)), top10_p)\n",
    "        axs[1].set_xticks(range(len(top10_p)))\n",
    "        axs[1].set_xticklabels(top10_class)\n",
    "        axs[1].set_title('grapheme_root: {}'.format(labels[0][i]))\n",
    "        \n",
    "        # analyze vowel prediction\n",
    "        ps_vowel = F.softmax(vowel_diacritic[i])\n",
    "        top11_p, top11_class = ps_vowel.topk(11, dim=0)\n",
    "        \n",
    "        top11_p = top11_p.detach().numpy()\n",
    "        top11_class = top11_class.detach().numpy()\n",
    "        \n",
    "        axs[2].bar(range(len(top11_p)), top11_p)\n",
    "        axs[2].set_xticks(range(len(top11_p)))\n",
    "        axs[2].set_xticklabels(top11_class)\n",
    "        axs[2].set_title('vowel_diacritic: {}'.format(labels[1][i]))\n",
    "        \n",
    "        # analyze consonant prediction\n",
    "        ps_cons = F.softmax(consonant_diacritic[i])\n",
    "        top7_p, top7_class = ps_cons.topk(7, dim=0)\n",
    "        \n",
    "        top7_p = top7_p.detach().numpy()\n",
    "        top7_class = top7_class.detach().numpy()\n",
    "        \n",
    "        axs[3].bar(range(len(top7_p)), top7_p)\n",
    "        axs[3].set_xticks(range(len(top7_p)))\n",
    "        axs[3].set_xticklabels(top7_class)\n",
    "        axs[3].set_title('consonant_diacritic: {}'.format(labels[2][i]))\n",
    "        \n",
    "        plt.show()\n",
    "        break;\n",
    "        \n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "test_dataset = BengaliDataset(test, valid_transforms(), test_labels, validation = True)\n",
    "sample_validloader = DataLoader(test_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the images from validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "for img, image_ids in sample_validloader:\n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        axs[i].set_title(image_ids[i])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_label(ps):\n",
    "    '''\n",
    "    Helper function to get the predicted label given the probabilities from the model output\n",
    "    '''\n",
    "    ps = F.softmax(ps)[0]\n",
    "    top_p, top_class = ps.topk(1, dim=0)\n",
    "        \n",
    "    top_p = top_p.detach().numpy()\n",
    "    top_class = top_class.detach().numpy()\n",
    "    \n",
    "    return top_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission\n",
    "# initialize the dataframe\n",
    "submission = pd.DataFrame(columns=['row_id', 'target'])\n",
    "\n",
    "for imgs, image_ids in validloader:\n",
    "    img = imgs[0]\n",
    "    image_id = image_ids[0]\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    # forward pass to get the output\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(imgs)\n",
    "    \n",
    "    imgs = imgs.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # get the predicted labels\n",
    "    grapheme_root_label = get_predicted_label(grapheme_root)\n",
    "    vowel_diacritic_label = get_predicted_label(vowel_diacritic)\n",
    "    consonant_diacritic_label = get_predicted_label(consonant_diacritic)\n",
    "    \n",
    "    # add the results to the dataframe\n",
    "    submission = submission.append({'row_id':str(image_id)+'_grapheme_root', 'target':grapheme_root_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_vowel_diacritic', 'target':vowel_diacritic_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_consonant_diacritic', 'target':consonant_diacritic_label}, \n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook I created and trained a sample model. This code can't be used for the actual predcitions for the competition. It requires a lot of optiomization, but you can use it as a sample for learning purposes.\n",
    "\n",
    "## References\n",
    "1. [EfficientNet paper](https://arxiv.org/pdf/1905.11946.pdf)\n",
    "2. [efficientnet-pytorch pacckage](https://pypi.org/project/efficientnet-pytorch/)\n",
    "3. [My EDA notebook for Bengali.AI](https://www.kaggle.com/aleksandradeis/bengali-ai-eda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

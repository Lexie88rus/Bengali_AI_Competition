{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from efficientnet_pytorch.utils import Conv2dStaticSamePadding, get_model_params\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "# import image augmentation\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,GaussianBlur,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, Flip, OneOf, Compose, PadIfNeeded, RandomContrast, RandomGamma, RandomBrightness, ElasticTransform,\n",
    "    CenterCrop, Resize\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"BengaliAI\"\n",
    "EXPERIMENT_NAME = \"DenseNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VERTA_EMAIL'] = 'astakhova.aleksandra@gmail.com'\n",
    "os.environ['VERTA_DEV_KEY'] = 'd7ee32b5-bbd0-4c4c-a2ec-a070848021be'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "set existing Project: BengaliAI\n",
      "set existing Experiment: DenseNet\n",
      "created new ExperimentRun: Run 3673215799773075208151\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "run.log_tag('DenseNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the input data folder\n",
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with labels\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_labels = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "class_map = pd.read_csv(DATA_PATH + 'class_map.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_type</th>\n",
       "      <th>label</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grapheme_root</td>\n",
       "      <td>0</td>\n",
       "      <td>ং</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grapheme_root</td>\n",
       "      <td>1</td>\n",
       "      <td>ঃ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grapheme_root</td>\n",
       "      <td>2</td>\n",
       "      <td>অ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grapheme_root</td>\n",
       "      <td>3</td>\n",
       "      <td>আ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grapheme_root</td>\n",
       "      <td>4</td>\n",
       "      <td>ই</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  component_type  label component\n",
       "0  grapheme_root      0         ং\n",
       "1  grapheme_root      1         ঃ\n",
       "2  grapheme_root      2         অ\n",
       "3  grapheme_root      3         আ\n",
       "4  grapheme_root      4         ই"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    '''\n",
    "    Helper function to load all train and test images\n",
    "    '''\n",
    "    train_list = []\n",
    "    for i in range(0,4):\n",
    "        train_list.append(pd.read_parquet(DATA_PATH + 'train_image_data_{}.parquet'.format(i)))\n",
    "    train = pd.concat(train_list, ignore_index=True)\n",
    "    \n",
    "    test_list = []\n",
    "    for i in range(0,4):\n",
    "        test_list.append(pd.read_parquet(DATA_PATH + 'test_image_data_{}.parquet'.format(i)))\n",
    "    test = pd.concat(test_list, ignore_index=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing and Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup image hight and width\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "SIZE = 32\n",
    "\n",
    "transform = transforms.Compose([ transforms.CenterCrop(SIZE), transforms.Resize(64), transforms.ToTensor()])\n",
    "\n",
    "train_transforms = albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.CenterCrop(height = 128, width = 128),\n",
    "        albu.Resize(height = SIZE, width = SIZE)\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the custom dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(idx, df, labels):\n",
    "    '''\n",
    "    Helper function to get the image and label from the training set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n",
    "    # get the labels\n",
    "    row = labels[labels.image_id == image_id]\n",
    "    labels = row['grapheme_root'].values[0], \\\n",
    "    row['vowel_diacritic'].values[0], \\\n",
    "    row['consonant_diacritic'].values[0]\n",
    "    \n",
    "    return img, labels\n",
    "\n",
    "def get_validation(idx, df):\n",
    "    '''\n",
    "    Helper function to get the validation image and image_id from the test set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n",
    "    return img, image_id\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    '''\n",
    "    Create custom Bengali dataset\n",
    "    '''\n",
    "    def __init__(self, df_images, transforms, df_labels = None, validation = False):\n",
    "        self.df_images = df_images\n",
    "        self.df_labels = df_labels\n",
    "        self.transform = transforms\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.validation:\n",
    "            img, label = get_image(idx, self.df_images, self.df_labels)\n",
    "            img = img.astype(np.uint8)\n",
    "            \n",
    "            if self.transform:\n",
    "                aug = self.transform(image = img)\n",
    "                img = aug['image']\n",
    "            \n",
    "            img = TF.to_tensor(img)\n",
    "            return img, label\n",
    "        else:\n",
    "            img, image_id = get_validation(idx, self.df_images)\n",
    "            img = img.astype(np.uint8)\n",
    "            \n",
    "            if self.transform:\n",
    "                aug = self.transform(image = img)\n",
    "                img = aug['image']\n",
    "            \n",
    "            img = TF.to_tensor(img)\n",
    "                \n",
    "            return img, image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get some images and labels from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "train_dataset = BengaliDataset(train, train_transforms, train_labels)\n",
    "sample_trainloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample train data\n",
    "for img, labels in sample_trainloader:\n",
    "    \n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('./kalpurush.ttf')\n",
    "        grapheme_root = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[i].set_title('{}, {}, {}'.format(grapheme_root, vowel_diacritic, consonant_diacritic), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dense net in pytorch\n",
    "[1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger.\n",
    "    Densely Connected Convolutional Networks\n",
    "    https://arxiv.org/abs/1608.06993v5\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "#\"\"\"Bottleneck layers. Although each layer only produces k\n",
    "#output feature-maps, it typically has many more inputs. It\n",
    "#has been noted in [37, 11] that a 1×1 convolution can be in-\n",
    "#troduced as bottleneck layer before each 3×3 convolution\n",
    "#to reduce the number of input feature-maps, and thus to\n",
    "#improve computational efficiency.\"\"\"\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super().__init__()\n",
    "        #\"\"\"In  our experiments, we let each 1×1 convolution \n",
    "        #produce 4k feature-maps.\"\"\"\n",
    "        inner_channel = 4 * growth_rate\n",
    "\n",
    "        #\"\"\"We find this design especially effective for DenseNet and \n",
    "        #we refer to our network with such a bottleneck layer, i.e., \n",
    "        #to the BN-ReLU-Conv(1×1)-BN-ReLU-Conv(3×3) version of H ` , \n",
    "        #as DenseNet-B.\"\"\"\n",
    "        self.bottle_neck = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, inner_channel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(inner_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inner_channel, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([x, self.bottle_neck(x)], 1)\n",
    "\n",
    "#\"\"\"We refer to layers between blocks as transition\n",
    "#layers, which do convolution and pooling.\"\"\"\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        #\"\"\"The transition layers used in our experiments \n",
    "        #consist of a batch normalization layer and an 1×1 \n",
    "        #convolutional layer followed by a 2×2 average pooling \n",
    "        #layer\"\"\".\n",
    "        self.down_sample = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.AvgPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_sample(x)\n",
    "\n",
    "#DesneNet-BC\n",
    "#B stands for bottleneck layer(BN-RELU-CONV(1x1)-BN-RELU-CONV(3x3))\n",
    "#C stands for compression factor(0<=theta<=1)\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_class=100):\n",
    "        super().__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        #\"\"\"Before entering the first dense block, a convolution \n",
    "        #with 16 (or twice the growth rate for DenseNet-BC) \n",
    "        #output channels is performed on the input images.\"\"\"\n",
    "        inner_channels = 2 * growth_rate\n",
    "\n",
    "        #For convolutional layers with kernel size 3×3, each \n",
    "        #side of the inputs is zero-padded by one pixel to keep \n",
    "        #the feature-map size fixed.\n",
    "        self.conv1 = nn.Conv2d(1, inner_channels, kernel_size=3, padding=1, bias=False) \n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "\n",
    "        for index in range(len(nblocks) - 1):\n",
    "            self.features.add_module(\"dense_block_layer_{}\".format(index), self._make_dense_layers(block, inner_channels, nblocks[index]))\n",
    "            inner_channels += growth_rate * nblocks[index]\n",
    "\n",
    "            #\"\"\"If a dense block contains m feature-maps, we let the \n",
    "            #following transition layer generate θm output feature-\n",
    "            #maps, where 0 < θ ≤ 1 is referred to as the compression \n",
    "            #fac-tor.\n",
    "            out_channels = int(reduction * inner_channels) # int() will automatic floor the value\n",
    "            self.features.add_module(\"transition_layer_{}\".format(index), Transition(inner_channels, out_channels))\n",
    "            inner_channels = out_channels\n",
    "\n",
    "        self.features.add_module(\"dense_block{}\".format(len(nblocks) - 1), self._make_dense_layers(block, inner_channels, nblocks[len(nblocks)-1]))\n",
    "        inner_channels += growth_rate * nblocks[len(nblocks) - 1]\n",
    "        self.features.add_module('bn', nn.BatchNorm2d(inner_channels))\n",
    "        self.features.add_module('relu', nn.ReLU(inplace=True))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.linear = nn.Linear(inner_channels, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.features(output)\n",
    "        output = self.avgpool(output)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    def _make_dense_layers(self, block, in_channels, nblocks):\n",
    "        dense_block = nn.Sequential()\n",
    "        for index in range(nblocks):\n",
    "            dense_block.add_module('bottle_neck_layer_{}'.format(index), block(in_channels, self.growth_rate))\n",
    "            in_channels += self.growth_rate\n",
    "        return dense_block\n",
    "\n",
    "def densenet121(num_class = 100):\n",
    "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32, num_class=num_class)\n",
    "\n",
    "def densenet169(num_class = 100):\n",
    "    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32, num_class=num_class)\n",
    "\n",
    "def densenet201(num_class = 100):\n",
    "    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32, num_class=num_class)\n",
    "\n",
    "def densenet161(num_class = 100):\n",
    "    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48, num_class=num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = densenet121(num_class = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliModel(nn.Module):\n",
    "    def __init__(self, backbone_model):\n",
    "        super(BengaliModel, self).__init__()\n",
    "        #self.conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)\n",
    "        self.backbone_model = backbone_model\n",
    "        self.fc1 = nn.Linear(in_features=1000, out_features=168) # grapheme_root\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=11) # vowel_diacritic\n",
    "        self.fc3 = nn.Linear(in_features=1000, out_features=7) # consonant_diacritic\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass through the backbone model\n",
    "        #y = self.conv(x)\n",
    "        y = self.backbone_model(x)\n",
    "        \n",
    "        # multi-output\n",
    "        grapheme_root = self.fc1(y)\n",
    "        vowel_diacritic = self.fc2(y)\n",
    "        consonant_diacritic = self.fc3(y)\n",
    "        \n",
    "        return grapheme_root, vowel_diacritic, consonant_diacritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengaliModel(backbone_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.2\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"test_split\", test_split)\n",
    "run.log_hyperparameter(\"batch_size\", batch_size)\n",
    "run.log_hyperparameter(\"epochs\", epochs)\n",
    "run.log_hyperparameter(\"learning_rate\", learning_rate)\n",
    "run.log_hyperparameter(\"image_size\", SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the samplers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "testloader = DataLoader(train_dataset, batch_size=32, sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizer, only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"optimizer\", \"Adam\")\n",
    "run.log_hyperparameter(\"loss\", \"CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup training device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dataframe to store training statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy'\n",
    "                                      ,'Test loss', 'Test accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(ps, labels):\n",
    "    ps = torch.exp(ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy = torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = torch.load('mobilenet_v2_10.pth', map_location=lambda storage, loc: storage)\n",
    "#model.load_state_dict(state[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\lexie\\lib\\site-packages\\ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d877dbc35fd942918ef10e661c1d8b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10.. Time per epoch: 6908.9251.. Average time per step: 2.7515.. Train loss: 2.4753.. Train accuracy: 0.7647.. Test loss: 1.1550.. Test accuracy: 0.8852.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cc79bb36da43c580e7f411118f147b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10.. Time per epoch: 6912.7034.. Average time per step: 2.7530.. Train loss: 0.9156.. Train accuracy: 0.9086.. Test loss: 0.8508.. Test accuracy: 0.9163.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48881623b903496d899f6f843f3250dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10.. Time per epoch: 6909.1276.. Average time per step: 2.7515.. Train loss: 0.7001.. Train accuracy: 0.9301.. Test loss: 0.7176.. Test accuracy: 0.9308.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabf65d2da204d73b5ec9c6d042e0960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "running_loss = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    train_accuracy = 0\n",
    "    top3_train_accuracy = 0 \n",
    "    for inputs, labels in tqdm_notebook(trainloader):\n",
    "        steps += 1\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "        loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + \\\n",
    "        criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        train_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "            \n",
    "            grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "            batch_loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "            # Calculate test top-1 accuracy\n",
    "            test_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "            \n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Time per epoch: {time_elapsed:.4f}.. \"\n",
    "          f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n",
    "          f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n",
    "          f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n",
    "          f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n",
    "          f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \")\n",
    "\n",
    "    train_stats = train_stats.append({'Epoch': epoch, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader),\n",
    "                                      'Train accuracy': train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader),\n",
    "                                      'Test accuracy': test_accuracy/len(testloader)}, ignore_index=True)\n",
    "    \n",
    "    filename = 'densenet121_'+ str(epoch+1) + '.pth'\n",
    "    checkpoint = {'state_dict': model.state_dict()}\n",
    "    torch.save(checkpoint, filename)\n",
    "    \n",
    "    run.log_observation(\"time_per_epoch\", time_elapsed)\n",
    "    run.log_observation(\"time_per_step\", time_elapsed/len(trainloader))\n",
    "    run.log_observation(\"train_loss\", running_loss/len(trainloader))\n",
    "    run.log_observation(\"test_loss\", test_loss/len(testloader))\n",
    "    run.log_observation(\"train_accuracy\", train_accuracy/len(trainloader))\n",
    "    run.log_observation(\"test_accuracy\", test_accuracy/len(testloader))\n",
    "\n",
    "    running_loss = 0\n",
    "    steps = 0\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights and training statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'densenet121_'+ str(epochs) + '.pth'\n",
    "\n",
    "checkpoint = {'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.to_csv('densenet121_train_stats_{}.csv'.format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('model', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('train_stats', train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_stats['Train loss'], label='train')\n",
    "plt.plot(train_stats['Test loss'], label='test')\n",
    "plt.title('Loss over epoch')\n",
    "plt.legend()\n",
    "\n",
    "run.log_image(\"loss\", plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_stats['Train accuracy'], label='train')\n",
    "plt.plot(train_stats['Test accuracy'], label='test')\n",
    "plt.title('Accuracy over epoch')\n",
    "plt.legend()\n",
    "\n",
    "run.log_image(\"accuracy\", plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "model.eval()\n",
    "for img, labels in testloader:\n",
    "    img, labels = img.to(device), [label.to(device) for label in labels]\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(img)\n",
    "    \n",
    "    img = img.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # visualize the inputs\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(10,15))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[0].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('./kalpurush.ttf')\n",
    "        grapheme_root_str = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic_str = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic_str = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[0].set_title('{}, {}, {}'.format(grapheme_root_str, vowel_diacritic_str, consonant_diacritic_str), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "        \n",
    "        # analyze grapheme root prediction\n",
    "        ps_root = F.softmax(grapheme_root[i])\n",
    "        top10_p, top10_class = ps_root.topk(10, dim=0)\n",
    "        \n",
    "        top10_p = top10_p.detach().numpy()\n",
    "        top10_class = top10_class.detach().numpy()\n",
    "        \n",
    "        axs[1].bar(range(len(top10_p)), top10_p)\n",
    "        axs[1].set_xticks(range(len(top10_p)))\n",
    "        axs[1].set_xticklabels(top10_class)\n",
    "        axs[1].set_title('grapheme_root: {}'.format(labels[0][i]))\n",
    "        \n",
    "        # analyze vowel prediction\n",
    "        ps_vowel = F.softmax(vowel_diacritic[i])\n",
    "        top11_p, top11_class = ps_vowel.topk(11, dim=0)\n",
    "        \n",
    "        top11_p = top11_p.detach().numpy()\n",
    "        top11_class = top11_class.detach().numpy()\n",
    "        \n",
    "        axs[2].bar(range(len(top11_p)), top11_p)\n",
    "        axs[2].set_xticks(range(len(top11_p)))\n",
    "        axs[2].set_xticklabels(top11_class)\n",
    "        axs[2].set_title('vowel_diacritic: {}'.format(labels[1][i]))\n",
    "        \n",
    "        # analyze consonant prediction\n",
    "        ps_cons = F.softmax(consonant_diacritic[i])\n",
    "        top7_p, top7_class = ps_cons.topk(7, dim=0)\n",
    "        \n",
    "        top7_p = top7_p.detach().numpy()\n",
    "        top7_class = top7_class.detach().numpy()\n",
    "        \n",
    "        axs[3].bar(range(len(top7_p)), top7_p)\n",
    "        axs[3].set_xticks(range(len(top7_p)))\n",
    "        axs[3].set_xticklabels(top7_class)\n",
    "        axs[3].set_title('consonant_diacritic: {}'.format(labels[2][i]))\n",
    "        \n",
    "        plt.show()\n",
    "        break;\n",
    "        \n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize test dataset and visualize sample test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "test_dataset = BengaliDataset(test, transform, test_labels, validation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_validloader = DataLoader(test_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "for img, image_ids in sample_validloader:\n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        axs[i].set_title(image_ids[i])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_label(ps):\n",
    "    ps = F.softmax(ps)[0]\n",
    "    top_p, top_class = ps.topk(1, dim=0)\n",
    "        \n",
    "    top_p = top_p.detach().numpy()\n",
    "    top_class = top_class.detach().numpy()\n",
    "    \n",
    "    return top_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=['row_id', 'target'])\n",
    "\n",
    "for imgs, image_ids in validloader:\n",
    "    img = imgs[0]\n",
    "    image_id = image_ids[0]\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(imgs)\n",
    "    \n",
    "    imgs = imgs.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    grapheme_root_label = get_predicted_label(grapheme_root)\n",
    "    vowel_diacritic_label = get_predicted_label(vowel_diacritic)\n",
    "    consonant_diacritic_label = get_predicted_label(consonant_diacritic)\n",
    "    \n",
    "    submission = submission.append({'row_id':str(image_id)+'_grapheme_root', 'target':grapheme_root_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_vowel_diacritic', 'target':vowel_diacritic_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_consonant_diacritic', 'target':consonant_diacritic_label}, \n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

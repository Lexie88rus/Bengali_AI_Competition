{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import time\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/Lexie88rus/Bengali_AI_Competition/raw/master/assets/samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the path to data and load the csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# setup the input data folder\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "# load the dataframes with labels\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_labels = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "class_map = pd.read_csv(DATA_PATH + 'class_map.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"BengaliAI\"\n",
    "EXPERIMENT_NAME = \"CNN-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VERTA_EMAIL'] = 'astakhova.aleksandra@gmail.com'\n",
    "os.environ['VERTA_DEV_KEY'] = 'd7ee32b5-bbd0-4c4c-a2ec-a070848021be'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "set existing Project: BengaliAI\n",
      "set existing Experiment: CNN-1\n",
      "created new ExperimentRun: Run 2491615803626895140433\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "run.log_tag('CNN-1')\n",
    "run.log_tag('Dropout')\n",
    "run.log_tag('reduce lr on plateau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test and train sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    '''\n",
    "    Helper function to load all train and test images\n",
    "    '''\n",
    "    train_list = []\n",
    "    for i in range(0,4):\n",
    "        train_list.append(pd.read_parquet(DATA_PATH + 'train_image_data_{}.parquet'.format(i)))\n",
    "    train = pd.concat(train_list, ignore_index=True)\n",
    "    \n",
    "    test_list = []\n",
    "    for i in range(0,4):\n",
    "        test_list.append(pd.read_parquet(DATA_PATH + 'test_image_data_{}.parquet'.format(i)))\n",
    "    test = pd.concat(test_list, ignore_index=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and data augmentation are exteremely important for the training of deep learning models. I use the adaptive thresholding to binarize the input images and a simple data augmentation pipeline consisting of random crop-resize and slight rotation of the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup image hight and width\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "\n",
    "SIZE = 32\n",
    "\n",
    "def threshold_image(img):\n",
    "    '''\n",
    "    Helper function for thresholding the images\n",
    "    '''\n",
    "    gray = PIL.Image.fromarray(np.uint8(img), 'L')\n",
    "    ret,th = cv.threshold(np.array(gray),0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    return th\n",
    "\n",
    "def train_transforms(p=.5):\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the cropping and random rotation\n",
    "        albu.RandomSizedCrop(min_max_height=(int(HEIGHT // 1.1), HEIGHT), height = HEIGHT, width = WIDTH, p=0.5),\n",
    "        albu.CenterCrop(height = 128, width = 128),\n",
    "        albu.Rotate(limit=5, p=p), # add rotation\n",
    "        albu.Resize(height = SIZE, width = SIZE)\n",
    "    ], p=1.0)\n",
    "\n",
    "def valid_transforms():\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.CenterCrop(height = 128, width = 128),\n",
    "        albu.Resize(height = SIZE, width = SIZE)\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a custom pytorch dataset, which will produce images and corresponding labels out of the traing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions to retrieve the images from the dataset in training and validation modes\n",
    "'''\n",
    "\n",
    "def get_image(idx, df, labels):\n",
    "    '''\n",
    "    Helper function to get the image and label from the training set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n",
    "    # get the labels\n",
    "    row = labels[labels.image_id == image_id]\n",
    "    \n",
    "    # return labels as tuple\n",
    "    labels = row['grapheme_root'].values[0], \\\n",
    "    row['vowel_diacritic'].values[0], \\\n",
    "    row['consonant_diacritic'].values[0]\n",
    "    \n",
    "    return img, labels\n",
    "\n",
    "def get_validation(idx, df):\n",
    "    '''\n",
    "    Helper function to get the validation image and image_id from the test set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n",
    "    return img, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDataset(Dataset):\n",
    "    '''\n",
    "    Create a custom Bengali images dataset\n",
    "    '''\n",
    "    def __init__(self, df_images, transforms, df_labels = None, validation = False):\n",
    "        '''\n",
    "        Init function\n",
    "        INPUT:\n",
    "            df_images - dataframe with the images\n",
    "            transforms - data transforms\n",
    "            df_labels - datafrane containing the target labels\n",
    "            validation - flag indication if the dataset is for training or for validation\n",
    "        '''\n",
    "        self.df_images = df_images\n",
    "        self.df_labels = df_labels\n",
    "        self.transforms = transforms\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.validation:\n",
    "            # get the image\n",
    "            img, label = get_image(idx, self.df_images, self.df_labels)\n",
    "            # threshold the image\n",
    "            img = threshold_image(img)\n",
    "            # transform the image\n",
    "            #img = img.astype(np.uint8)\n",
    "            aug = self.transforms(image = img)\n",
    "            return TF.to_tensor(aug['image']), label\n",
    "        else:\n",
    "            # get the image\n",
    "            img, image_id = get_validation(idx, self.df_images)\n",
    "            # threshold the image\n",
    "            img = threshold_image(img)\n",
    "            # transform the image\n",
    "            #img = img.astype(np.uint8)\n",
    "            aug = self.transforms(image = img)\n",
    "            # return transformed image and corresponding image_id (instead of label) to create submission\n",
    "            return TF.to_tensor(aug['image']), image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that everything is correct. Let's try to retrieve couple of images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "train_dataset = BengaliDataset(train, train_transforms(), train_labels)\n",
    "# create a sample trainloader\n",
    "sample_trainloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample train data\n",
    "for img, labels in sample_trainloader:\n",
    "    \n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('./kalpurush.ttf')\n",
    "        grapheme_root = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[i].set_title('{}, {}, {}'.format(grapheme_root, vowel_diacritic, consonant_diacritic), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BengaliModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=16384, out_features=1024)\n",
    "        \n",
    "        self.dp1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512) \n",
    "        \n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=168) # grapheme_root\n",
    "        self.fc4 = nn.Linear(in_features=512, out_features=11) # vowel_diacritic\n",
    "        self.fc5 = nn.Linear(in_features=512, out_features=7) # consonant_diacritic\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.conv1(x))\n",
    "        \n",
    "        y = F.relu(self.bn1(self.conv2(y)))\n",
    "        \n",
    "        y = self.pool1(y)\n",
    "        \n",
    "        y = F.relu(self.conv3(y))\n",
    "        \n",
    "        y = F.relu(self.conv4(y))\n",
    "        \n",
    "        y = self.pool2(y)\n",
    "        \n",
    "        y = F.relu(self.conv5(y))\n",
    "        \n",
    "        # flatten\n",
    "        y = y.reshape(y.size(0), -1)\n",
    "        \n",
    "        y = F.relu(self.fc1(y))\n",
    "        \n",
    "        y = self.dp1(y)\n",
    "        \n",
    "        y = F.relu(self.fc2(y))\n",
    "        \n",
    "        # multi-output\n",
    "        grapheme_root = self.fc3(y)\n",
    "        vowel_diacritic = self.fc4(y)\n",
    "        consonant_diacritic = self.fc5(y)\n",
    "        \n",
    "        return grapheme_root, vowel_diacritic, consonant_diacritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the final model\n",
    "model = BengaliModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set for the modelling:\n",
    "\n",
    "First, let's start with defining the hyperparameters. In this notebook I won't be actually training the model, that is why the number of epochs is 0. I trained the model on my own machine and will just load the weights here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.2\n",
    "batch_size = 128\n",
    "epochs = 10 # change this value to actually train the model\n",
    "learning_rate = 0.001\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"test_split\", test_split)\n",
    "run.log_hyperparameter(\"batch_size\", batch_size)\n",
    "run.log_hyperparameter(\"epochs\", epochs)\n",
    "run.log_hyperparameter(\"learning_rate\", learning_rate)\n",
    "run.log_hyperparameter(\"image_size\", SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the dataset and samplers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "\n",
    "# split the dataset into test and train\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "testloader = DataLoader(train_dataset, batch_size=32, sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizer, only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"optimizer\", \"Adam\")\n",
    "run.log_hyperparameter(\"loss\", \"CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup training device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the logging. I will write the log into pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy'\n",
    "                                      ,'Test loss', 'Test accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to the training device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# I'm just loading the weights instead of training\n",
    "#state = torch.load('../input/bengaliaiutils/efficientnet_b0_10.pth', map_location=lambda storage, loc: storage)\n",
    "#model.load_state_dict(state[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|‚ñç                                                                              | 6/1256 [00:35<2:03:34,  5.93s/it]"
     ]
    }
   ],
   "source": [
    "def get_accuracy(ps, labels):\n",
    "    '''\n",
    "    Helper function to calculate the accuracy given the labels and the output of the model\n",
    "    '''\n",
    "    ps = torch.exp(ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy = torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    return accuracy\n",
    "\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    train_accuracy = 0\n",
    "    top3_train_accuracy = 0 \n",
    "    for inputs, labels in tqdm(trainloader):\n",
    "        steps += 1\n",
    "        # move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + \\\n",
    "        criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # get the average accuracy\n",
    "        train_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "        \n",
    "\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    model.eval()\n",
    "    # run validation on the test set\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "            \n",
    "            grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "            batch_loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "            test_loss += batch_loss.item()\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Calculate test top-1 accuracy\n",
    "            test_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "    \n",
    "    # print out the training stats\n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Time per epoch: {time_elapsed:.4f}.. \"\n",
    "          f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n",
    "          f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n",
    "          f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n",
    "          f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n",
    "          f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \")\n",
    "\n",
    "    # write to the training log\n",
    "    train_stats = train_stats.append({'Epoch': epoch, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader),\n",
    "                                      'Train accuracy': train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader),\n",
    "                                      'Test accuracy': test_accuracy/len(testloader)}, ignore_index=True)\n",
    "    \n",
    "    filename = 'cnn-1_dropout_'+ str(epoch+1) + '.pth'\n",
    "    checkpoint = {'state_dict': model.state_dict()}\n",
    "    torch.save(checkpoint, filename)\n",
    "    \n",
    "    run.log_observation(\"time_per_epoch\", time_elapsed)\n",
    "    run.log_observation(\"time_per_step\", time_elapsed/len(trainloader))\n",
    "    run.log_observation(\"train_loss\", running_loss/len(trainloader))\n",
    "    run.log_observation(\"test_loss\", test_loss/len(testloader))\n",
    "    run.log_observation(\"train_accuracy\", train_accuracy/len(trainloader))\n",
    "    run.log_observation(\"test_accuracy\", test_accuracy/len(testloader))\n",
    "\n",
    "    running_loss = 0\n",
    "    steps = 0\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'cnn1_dropout_'+ str(epochs) + '.pth'\n",
    "\n",
    "checkpoint = {'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.to_csv('cnn1_dropout_train_stats_{}.csv'.format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('model', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('train_stats', train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(train_stats['Train loss'], label='train')\n",
    "plt.plot(train_stats['Test loss'], label='test')\n",
    "plt.title('Loss over epoch')\n",
    "plt.legend()\n",
    "\n",
    "run.log_image(\"loss\", plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy\n",
    "plt.plot(train_stats['Train accuracy'], label='train')\n",
    "plt.plot(train_stats['Test accuracy'], label='test')\n",
    "plt.title('Accuracy over epoch')\n",
    "plt.legend()\n",
    "\n",
    "run.log_image(\"accuracy\", plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize some sample predictions from the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "model.eval()\n",
    "for img, labels in testloader:\n",
    "    img, labels = img.to(device), [label.to(device) for label in labels]\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(img)\n",
    "    \n",
    "    img = img.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # visualize the inputs\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(10,15))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[0].imshow(TF.to_pil_image(img[i].reshape(HEIGHT, WIDTH)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('./kalpurush.ttf')\n",
    "        grapheme_root_str = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic_str = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic_str = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[0].set_title('{}, {}, {}'.format(grapheme_root_str, vowel_diacritic_str, consonant_diacritic_str), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "        \n",
    "        # analyze grapheme root prediction\n",
    "        ps_root = F.softmax(grapheme_root[i])\n",
    "        top10_p, top10_class = ps_root.topk(10, dim=0)\n",
    "        \n",
    "        top10_p = top10_p.detach().numpy()\n",
    "        top10_class = top10_class.detach().numpy()\n",
    "        \n",
    "        axs[1].bar(range(len(top10_p)), top10_p)\n",
    "        axs[1].set_xticks(range(len(top10_p)))\n",
    "        axs[1].set_xticklabels(top10_class)\n",
    "        axs[1].set_title('grapheme_root: {}'.format(labels[0][i]))\n",
    "        \n",
    "        # analyze vowel prediction\n",
    "        ps_vowel = F.softmax(vowel_diacritic[i])\n",
    "        top11_p, top11_class = ps_vowel.topk(11, dim=0)\n",
    "        \n",
    "        top11_p = top11_p.detach().numpy()\n",
    "        top11_class = top11_class.detach().numpy()\n",
    "        \n",
    "        axs[2].bar(range(len(top11_p)), top11_p)\n",
    "        axs[2].set_xticks(range(len(top11_p)))\n",
    "        axs[2].set_xticklabels(top11_class)\n",
    "        axs[2].set_title('vowel_diacritic: {}'.format(labels[1][i]))\n",
    "        \n",
    "        # analyze consonant prediction\n",
    "        ps_cons = F.softmax(consonant_diacritic[i])\n",
    "        top7_p, top7_class = ps_cons.topk(7, dim=0)\n",
    "        \n",
    "        top7_p = top7_p.detach().numpy()\n",
    "        top7_class = top7_class.detach().numpy()\n",
    "        \n",
    "        axs[3].bar(range(len(top7_p)), top7_p)\n",
    "        axs[3].set_xticks(range(len(top7_p)))\n",
    "        axs[3].set_xticklabels(top7_class)\n",
    "        axs[3].set_title('consonant_diacritic: {}'.format(labels[2][i]))\n",
    "        \n",
    "        plt.show()\n",
    "        break;\n",
    "        \n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "test_dataset = BengaliDataset(test, valid_transforms(), test_labels, validation = True)\n",
    "sample_validloader = DataLoader(test_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the images from validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "for img, image_ids in sample_validloader:\n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        axs[i].set_title(image_ids[i])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_label(ps):\n",
    "    '''\n",
    "    Helper function to get the predicted label given the probabilities from the model output\n",
    "    '''\n",
    "    ps = F.softmax(ps)[0]\n",
    "    top_p, top_class = ps.topk(1, dim=0)\n",
    "        \n",
    "    top_p = top_p.detach().numpy()\n",
    "    top_class = top_class.detach().numpy()\n",
    "    \n",
    "    return top_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission\n",
    "# initialize the dataframe\n",
    "submission = pd.DataFrame(columns=['row_id', 'target'])\n",
    "\n",
    "for imgs, image_ids in validloader:\n",
    "    img = imgs[0]\n",
    "    image_id = image_ids[0]\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    # forward pass to get the output\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(imgs)\n",
    "    \n",
    "    imgs = imgs.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # get the predicted labels\n",
    "    grapheme_root_label = get_predicted_label(grapheme_root)\n",
    "    vowel_diacritic_label = get_predicted_label(vowel_diacritic)\n",
    "    consonant_diacritic_label = get_predicted_label(consonant_diacritic)\n",
    "    \n",
    "    # add the results to the dataframe\n",
    "    submission = submission.append({'row_id':str(image_id)+'_grapheme_root', 'target':grapheme_root_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_vowel_diacritic', 'target':vowel_diacritic_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_consonant_diacritic', 'target':consonant_diacritic_label}, \n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook I created and trained a sample model. This code can't be used for the actual predcitions for the competition. It requires a lot of optiomization, but you can use it as a sample for learning purposes.\n",
    "\n",
    "## References\n",
    "1. [EfficientNet paper](https://arxiv.org/pdf/1905.11946.pdf)\n",
    "2. [efficientnet-pytorch pacckage](https://pypi.org/project/efficientnet-pytorch/)\n",
    "3. [My EDA notebook for Bengali.AI](https://www.kaggle.com/aleksandradeis/bengali-ai-eda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

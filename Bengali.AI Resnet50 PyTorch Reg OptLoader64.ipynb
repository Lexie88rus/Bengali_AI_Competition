{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import time\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/Lexie88rus/Bengali_AI_Competition/raw/master/assets/samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengali.AI Resnet CutMix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the path to data and load the csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# setup the input data folder\n",
    "DATA_PATH = './data/'\n",
    "PREPROCESSED_PATH = './preprocessed64/'\n",
    "\n",
    "# load the dataframes with labels\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_labels = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "class_map = pd.read_csv(DATA_PATH + 'class_map.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"BengaliAI\"\n",
    "EXPERIMENT_NAME = \"Resnet50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VERTA_EMAIL'] = 'astakhova.aleksandra@gmail.com'\n",
    "os.environ['VERTA_DEV_KEY'] = 'd7ee32b5-bbd0-4c4c-a2ec-a070848021be'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "set existing Project: BengaliAI\n",
      "created new Experiment: Resnet50\n",
      "created new ExperimentRun: Run 1947215821267611130042\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "run.log_tag('Resnet34')\n",
    "run.log_tag('CutMix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and data augmentation are exteremely important for the training of deep learning models. I use the adaptive thresholding to binarize the input images and a simple data augmentation pipeline consisting of random crop-resize and slight rotation of the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup image hight and width\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "\n",
    "SIZE = 64\n",
    "\n",
    "def threshold_image(img):\n",
    "    '''\n",
    "    Helper function for thresholding the images\n",
    "    '''\n",
    "    gray = PIL.Image.fromarray(np.uint8(img), 'L')\n",
    "    ret,th = cv.threshold(np.array(gray),0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    return th\n",
    "\n",
    "def train_transforms(p=.5):\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.Rotate(limit=3, p=p),\n",
    "    ], p=1.0)\n",
    "\n",
    "def valid_transforms():\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.CenterCrop(height = 128, width = 128),\n",
    "        albu.Resize(height = SIZE, width = SIZE)\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a custom pytorch dataset, which will produce images and corresponding labels out of the traing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions to retrieve the images from the dataset in training and validation modes\n",
    "'''\n",
    "\n",
    "def get_image(idx, labels):\n",
    "    '''\n",
    "    Helper function to get the image and label from the training set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = labels.iloc[idx].image_id\n",
    "    filename = PREPROCESSED_PATH + str(image_id) + '.png' \n",
    "    # get the image by id\n",
    "    img = np.asarray(PIL.Image.open(filename))\n",
    "    # get the labels\n",
    "    row = labels[labels.image_id == image_id]\n",
    "    \n",
    "    # return labels as tuple\n",
    "    labels = row['grapheme_root'].values[0], \\\n",
    "    row['vowel_diacritic'].values[0], \\\n",
    "    row['consonant_diacritic'].values[0]\n",
    "    \n",
    "    return img, labels\n",
    "\n",
    "def get_validation(idx, labels):\n",
    "    '''\n",
    "    Helper function to get the validation image and image_id from the test set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = labels.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    filename = PREPROCESSED_PATH + str(image_id) + '.png' \n",
    "    # get the image by id\n",
    "    img = np.asarray(PIL.Image.open(filename))\n",
    "    return img, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDataset(Dataset):\n",
    "    '''\n",
    "    Create a custom Bengali images dataset\n",
    "    '''\n",
    "    def __init__(self, transforms, df_labels = None, validation = False):\n",
    "        '''\n",
    "        Init function\n",
    "        INPUT:\n",
    "            df_images - dataframe with the images\n",
    "            transforms - data transforms\n",
    "            df_labels - datafrane containing the target labels\n",
    "            validation - flag indication if the dataset is for training or for validation\n",
    "        '''\n",
    "        self.df_labels = df_labels\n",
    "        self.transforms = transforms\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.validation:\n",
    "            # get the image\n",
    "            img, label = get_image(idx, self.df_labels)\n",
    "            # transform the image\n",
    "            aug = self.transforms(image = img)\n",
    "            img = TF.to_tensor(aug['image'])\n",
    "            #img = np.tile(img, (3,1,1))\n",
    "            return img, label\n",
    "        else:\n",
    "            # get the image\n",
    "            img, image_id = get_validation(idx, self.df_labels)\n",
    "            # transform the image\n",
    "            #img = np.tile(img, (3,1,1))\n",
    "            return img, image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that everything is correct. Let's try to retrieve couple of images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "train_dataset = BengaliDataset(train_transforms(), train_labels)\n",
    "# create a sample trainloader\n",
    "sample_trainloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample train data\n",
    "for img, labels in sample_trainloader:\n",
    "    \n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i][0].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('./kalpurush.ttf')\n",
    "        grapheme_root = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[i].set_title('{}, {}, {}'.format(grapheme_root, vowel_diacritic, consonant_diacritic), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * Bottleneck.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * Bottleneck.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * Bottleneck.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * Bottleneck.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, dataset, depth, num_classes, bottleneck=False):\n",
    "        super(ResNet, self).__init__()        \n",
    "        self.dataset = dataset\n",
    "        if self.dataset.startswith('cifar'):\n",
    "            self.inplanes = 16\n",
    "            print(bottleneck)\n",
    "            if bottleneck == True:\n",
    "                n = int((depth - 2) / 9)\n",
    "                block = Bottleneck\n",
    "            else:\n",
    "                n = int((depth - 2) / 6)\n",
    "                block = BasicBlock\n",
    "\n",
    "            self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.layer1 = self._make_layer(block, 16, n)\n",
    "            self.layer2 = self._make_layer(block, 32, n, stride=2)\n",
    "            self.layer3 = self._make_layer(block, 64, n, stride=2) \n",
    "            self.avgpool = nn.AvgPool2d(8)\n",
    "            self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        elif dataset == 'imagenet':\n",
    "            blocks ={18: BasicBlock, 34: BasicBlock, 50: Bottleneck, 101: Bottleneck, 152: Bottleneck, 200: Bottleneck}\n",
    "            layers ={18: [2, 2, 2, 2], 34: [3, 4, 6, 3], 50: [3, 4, 6, 3], 101: [3, 4, 23, 3], 152: [3, 8, 36, 3], 200: [3, 24, 36, 3]}\n",
    "            assert layers[depth], 'invalid detph for ResNet (depth should be one of 18, 34, 50, 101, 152, and 200)'\n",
    "\n",
    "            self.inplanes = 64\n",
    "            self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            self.layer1 = self._make_layer(blocks[depth], 64, layers[depth][0])\n",
    "            self.layer2 = self._make_layer(blocks[depth], 128, layers[depth][1], stride=2)\n",
    "            self.layer3 = self._make_layer(blocks[depth], 256, layers[depth][2], stride=2)\n",
    "            self.layer4 = self._make_layer(blocks[depth], 512, layers[depth][3], stride=2)\n",
    "            self.avgpool = nn.AvgPool2d(7) \n",
    "            self.fc = nn.Linear(512 * blocks[depth].expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.dataset == 'cifar10' or self.dataset == 'cifar100':\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "\n",
    "            x = self.avgpool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        elif self.dataset == 'imagenet':\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "\n",
    "            x = self.avgpool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "backbone_model = ResNet('cifar100', depth=50, num_classes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom model for the Bengali images\n",
    "backbone model may be replaced with any other archirtecture :)\n",
    "'''\n",
    "\n",
    "class BengaliModel(nn.Module):\n",
    "    def __init__(self, backbone_model):\n",
    "        super(BengaliModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.backbone_model = backbone_model\n",
    "        self.fc1 = nn.Linear(in_features=500, out_features=168) # grapheme_root\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=11) # vowel_diacritic\n",
    "        self.fc3 = nn.Linear(in_features=500, out_features=7) # consonant_diacritic\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass through the backbone model\n",
    "        y = self.conv(x)\n",
    "        y = self.pool(y)\n",
    "        \n",
    "        y = self.backbone_model(y)\n",
    "        \n",
    "        # multi-output\n",
    "        grapheme_root = self.fc1(y)\n",
    "        vowel_diacritic = self.fc2(y)\n",
    "        consonant_diacritic = self.fc3(y)\n",
    "        \n",
    "        return grapheme_root, vowel_diacritic, consonant_diacritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the final model\n",
    "model = BengaliModel(backbone_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set for the modelling:\n",
    "\n",
    "First, let's start with defining the hyperparameters. In this notebook I won't be actually training the model, that is why the number of epochs is 0. I trained the model on my own machine and will just load the weights here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.2\n",
    "batch_size = 128\n",
    "epochs = 50 # change this value to actually train the model\n",
    "learning_rate = 0.00001\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"test_split\", test_split)\n",
    "run.log_hyperparameter(\"batch_size\", batch_size)\n",
    "run.log_hyperparameter(\"epochs\", epochs)\n",
    "run.log_hyperparameter(\"learning_rate\", learning_rate)\n",
    "run.log_hyperparameter(\"image_size\", SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the dataset and samplers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "\n",
    "# split the dataset into test and train\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "testloader = DataLoader(train_dataset, batch_size=32, sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizer, only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"optimizer\", \"Adam\")\n",
    "run.log_hyperparameter(\"loss\", \"CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup training device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the logging. I will write the log into pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy'\n",
    "                                      ,'Test loss', 'Test accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to the training device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, learning_rate, epochs):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // (epochs * 0.5))) * (0.1 ** (epoch // (epochs * 0.75)))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "def get_learning_rate(optimizer):\n",
    "    lr = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr += [param_group['lr']]\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\lexie\\lib\\site-packages\\ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f018b5e29f3f4fc797e66a8376e88714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50.. Time per epoch: 1549.9328.. Average time per step: 1.2340.. Train loss: 0.0276.. Train accuracy: 0.9974.. Test loss: 1.0342.. Test accuracy: 0.9455.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6774328d1154dafa1409d942d6f49f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50.. Time per epoch: 1549.5645.. Average time per step: 1.2337.. Train loss: 0.0224.. Train accuracy: 0.9981.. Test loss: 1.0338.. Test accuracy: 0.9453.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8674d118000b4630a4166ade8eac44b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50.. Time per epoch: 1602.3107.. Average time per step: 1.2757.. Train loss: 0.0222.. Train accuracy: 0.9981.. Test loss: 1.0542.. Test accuracy: 0.9455.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26ab6b77e094f24a65051fb4d47c5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_accuracy(ps, labels):\n",
    "    '''\n",
    "    Helper function to calculate the accuracy given the labels and the output of the model\n",
    "    '''\n",
    "    ps = torch.exp(ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy = torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    return accuracy\n",
    "\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    train_accuracy = 0\n",
    "    top3_train_accuracy = 0 \n",
    "    for inputs, labels in tqdm_notebook(trainloader):\n",
    "        steps += 1\n",
    "        # move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + \\\n",
    "        criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # get the average accuracy\n",
    "        train_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch, learning_rate, epochs)\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    model.eval()\n",
    "    # run validation on the test set\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "            \n",
    "            grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "            batch_loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "            test_loss += batch_loss.item()\n",
    "            \n",
    "            scheduler.step(test_loss)\n",
    "\n",
    "            # Calculate test top-1 accuracy\n",
    "            test_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "    \n",
    "    # print out the training stats\n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Time per epoch: {time_elapsed:.4f}.. \"\n",
    "          f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n",
    "          f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n",
    "          f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n",
    "          f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n",
    "          f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \")\n",
    "    \n",
    "    filename = 'resnet50_cutmix_64_lr_'+ str(epoch+1) + '.pth'\n",
    "    checkpoint = {'state_dict': model.state_dict()}\n",
    "    torch.save(checkpoint, filename)\n",
    "    \n",
    "    run.log_observation(\"time_per_epoch\", time_elapsed)\n",
    "    run.log_observation(\"time_per_step\", time_elapsed/len(trainloader))\n",
    "    run.log_observation(\"train_loss\", running_loss/len(trainloader))\n",
    "    run.log_observation(\"test_loss\", test_loss/len(testloader))\n",
    "    run.log_observation(\"train_accuracy\", train_accuracy/len(trainloader))\n",
    "    run.log_observation(\"test_accuracy\", test_accuracy/len(testloader))\n",
    "    run.log_observation(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # write to the training log\n",
    "    train_stats = train_stats.append({'Epoch': epoch, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader),\n",
    "                                      'Train accuracy': train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader),\n",
    "                                      'Test accuracy': test_accuracy/len(testloader)}, ignore_index=True)\n",
    "\n",
    "    running_loss = 0\n",
    "    steps = 0\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'resnet50_cutmix_64_'+ str(epochs) + '.pth'\n",
    "\n",
    "checkpoint = {'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.to_csv('resnet50_cutmix_64_train_stats_{}.csv'.format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('model', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('train_stats', train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(train_stats['Train loss'], label='train')\n",
    "plt.plot(train_stats['Test loss'], label='test')\n",
    "plt.title('Loss over epoch')\n",
    "plt.legend()\n",
    "run.log_image(\"loss\", plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy\n",
    "plt.plot(train_stats['Train accuracy'], label='train')\n",
    "plt.plot(train_stats['Test accuracy'], label='test')\n",
    "plt.title('Accuracy over epoch')\n",
    "plt.legend()\n",
    "run.log_image(\"accuracy\", plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize some sample predictions from the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "model.eval()\n",
    "for img, labels in testloader:\n",
    "    img, labels = img.to(device), [label.to(device) for label in labels]\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(img)\n",
    "    \n",
    "    img = img.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # visualize the inputs\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(10,15))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[0].imshow(TF.to_pil_image(img[i].reshape(HEIGHT, WIDTH)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('../input/bengaliaiutils/kalpurush.ttf')\n",
    "        grapheme_root_str = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic_str = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic_str = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[0].set_title('{}, {}, {}'.format(grapheme_root_str, vowel_diacritic_str, consonant_diacritic_str), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "        \n",
    "        # analyze grapheme root prediction\n",
    "        ps_root = F.softmax(grapheme_root[i])\n",
    "        top10_p, top10_class = ps_root.topk(10, dim=0)\n",
    "        \n",
    "        top10_p = top10_p.detach().numpy()\n",
    "        top10_class = top10_class.detach().numpy()\n",
    "        \n",
    "        axs[1].bar(range(len(top10_p)), top10_p)\n",
    "        axs[1].set_xticks(range(len(top10_p)))\n",
    "        axs[1].set_xticklabels(top10_class)\n",
    "        axs[1].set_title('grapheme_root: {}'.format(labels[0][i]))\n",
    "        \n",
    "        # analyze vowel prediction\n",
    "        ps_vowel = F.softmax(vowel_diacritic[i])\n",
    "        top11_p, top11_class = ps_vowel.topk(11, dim=0)\n",
    "        \n",
    "        top11_p = top11_p.detach().numpy()\n",
    "        top11_class = top11_class.detach().numpy()\n",
    "        \n",
    "        axs[2].bar(range(len(top11_p)), top11_p)\n",
    "        axs[2].set_xticks(range(len(top11_p)))\n",
    "        axs[2].set_xticklabels(top11_class)\n",
    "        axs[2].set_title('vowel_diacritic: {}'.format(labels[1][i]))\n",
    "        \n",
    "        # analyze consonant prediction\n",
    "        ps_cons = F.softmax(consonant_diacritic[i])\n",
    "        top7_p, top7_class = ps_cons.topk(7, dim=0)\n",
    "        \n",
    "        top7_p = top7_p.detach().numpy()\n",
    "        top7_class = top7_class.detach().numpy()\n",
    "        \n",
    "        axs[3].bar(range(len(top7_p)), top7_p)\n",
    "        axs[3].set_xticks(range(len(top7_p)))\n",
    "        axs[3].set_xticklabels(top7_class)\n",
    "        axs[3].set_title('consonant_diacritic: {}'.format(labels[2][i]))\n",
    "        \n",
    "        plt.show()\n",
    "        break;\n",
    "        \n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "test_dataset = BengaliDataset(test, valid_transforms(), test_labels, validation = True)\n",
    "sample_validloader = DataLoader(test_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the images from validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "for img, image_ids in sample_validloader:\n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        axs[i].set_title(image_ids[i])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_label(ps):\n",
    "    '''\n",
    "    Helper function to get the predicted label given the probabilities from the model output\n",
    "    '''\n",
    "    ps = F.softmax(ps)[0]\n",
    "    top_p, top_class = ps.topk(1, dim=0)\n",
    "        \n",
    "    top_p = top_p.detach().numpy()\n",
    "    top_class = top_class.detach().numpy()\n",
    "    \n",
    "    return top_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission\n",
    "# initialize the dataframe\n",
    "submission = pd.DataFrame(columns=['row_id', 'target'])\n",
    "\n",
    "for imgs, image_ids in validloader:\n",
    "    img = imgs[0]\n",
    "    image_id = image_ids[0]\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    # forward pass to get the output\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(imgs)\n",
    "    \n",
    "    imgs = imgs.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # get the predicted labels\n",
    "    grapheme_root_label = get_predicted_label(grapheme_root)\n",
    "    vowel_diacritic_label = get_predicted_label(vowel_diacritic)\n",
    "    consonant_diacritic_label = get_predicted_label(consonant_diacritic)\n",
    "    \n",
    "    # add the results to the dataframe\n",
    "    submission = submission.append({'row_id':str(image_id)+'_grapheme_root', 'target':grapheme_root_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_vowel_diacritic', 'target':vowel_diacritic_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_consonant_diacritic', 'target':consonant_diacritic_label}, \n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook I created and trained a sample model. This code can't be used for the actual predcitions for the competition. It requires a lot of optiomization, but you can use it as a sample for learning purposes.\n",
    "\n",
    "## References\n",
    "1. [EfficientNet paper](https://arxiv.org/pdf/1905.11946.pdf)\n",
    "2. [efficientnet-pytorch pacckage](https://pypi.org/project/efficientnet-pytorch/)\n",
    "3. [My EDA notebook for Bengali.AI](https://www.kaggle.com/aleksandradeis/bengali-ai-eda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

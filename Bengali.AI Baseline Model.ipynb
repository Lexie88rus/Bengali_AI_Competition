{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the input data folder\n",
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with labels\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_labels = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "class_map = pd.read_csv(DATA_PATH + 'class_map.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    '''\n",
    "    Helper function to load all train and test images\n",
    "    '''\n",
    "    train_list = []\n",
    "    for i in range(0,4):\n",
    "        train_list.append(pd.read_parquet(DATA_PATH + 'train_image_data_{}.parquet'.format(i)))\n",
    "    train = pd.concat(train_list, ignore_index=True)\n",
    "    \n",
    "    test_list = []\n",
    "    for i in range(0,4):\n",
    "        test_list.append(pd.read_parquet(DATA_PATH + 'test_image_data_{}.parquet'.format(i)))\n",
    "    test = pd.concat(test_list, ignore_index=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing and Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def threshold_image(img):\n",
    "    '''\n",
    "    Helper function for thresholding the images\n",
    "    '''\n",
    "    gray = PIL.Image.fromarray(np.uint8(img), 'L')\n",
    "    ret,th = cv.threshold(np.array(gray),0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    return th\n",
    "\n",
    "def train_transforms(p=.5):\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.RandomSizedCrop(min_max_height=(int(HEIGHT // 1.1), HEIGHT), height = HEIGHT, width = WIDTH, p=p),\n",
    "        albu.Rotate(limit=5, p=p)\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup image hight and width\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "\n",
    "def get_image(idx, df, labels):\n",
    "    '''\n",
    "    Helper function to get the image and label from the training set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n",
    "    # get the label\n",
    "    row = labels[labels.image_id == image_id]\n",
    "    label = torch.tensor([row['grapheme_root'].values[0], \n",
    "                          row['vowel_diacritic'].values[0], \n",
    "                          row['consonant_diacritic'].values[0]])\n",
    "    return img, label\n",
    "\n",
    "def get_validation(idx, df):\n",
    "    '''\n",
    "    Helper function to get the validation image and image_id from the test set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n",
    "    return img, image_id\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    '''\n",
    "    Create custom Bengali dataset\n",
    "    '''\n",
    "    def __init__(self, df_images, transforms, df_labels = None, validation = False):\n",
    "        self.df_images = df_images\n",
    "        self.df_labels = df_labels\n",
    "        self.transforms = transforms\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.validation:\n",
    "            img, label = get_image(idx, self.df_images, self.df_labels)\n",
    "            img = threshold_image(img)\n",
    "            aug = self.transforms(image = img)\n",
    "            return TF.to_tensor(aug['image']), label\n",
    "        else:\n",
    "            img, image_id = get_validation(idx, self.df_images)\n",
    "            img = threshold_image(img)\n",
    "            return TF.to_tensor(img), image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "train_dataset = BengaliDataset(train, train_transforms(), train_labels)\n",
    "sample_trainloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample train data\n",
    "for img, label in sample_trainloader:\n",
    "    plt.axis('off')\n",
    "    plt.imshow(TF.to_pil_image(img.reshape(HEIGHT, WIDTH)), cmap='gray')\n",
    "\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

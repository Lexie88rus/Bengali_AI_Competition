{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import time\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/Lexie88rus/Bengali_AI_Competition/raw/master/assets/samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengali.AI Resnet CutMix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the path to data and load the csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# setup the input data folder\n",
    "DATA_PATH = './data/'\n",
    "PREPROCESSED_PATH = './preprocessed64/'\n",
    "\n",
    "# load the dataframes with labels\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_labels = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "class_map = pd.read_csv(DATA_PATH + 'class_map.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"BengaliAI\"\n",
    "EXPERIMENT_NAME = \"SEResnext50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VERTA_EMAIL'] = 'astakhova.aleksandra@gmail.com'\n",
    "os.environ['VERTA_DEV_KEY'] = 'd7ee32b5-bbd0-4c4c-a2ec-a070848021be'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "set existing Project: BengaliAI\n",
      "set existing Experiment: SEResnext50\n",
      "created new ExperimentRun: Run 2879215830951377302969\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "run.log_tag('Resnet50')\n",
    "run.log_tag('cifar')\n",
    "run.log_tag('added more transforms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and data augmentation are exteremely important for the training of deep learning models. I use the adaptive thresholding to binarize the input images and a simple data augmentation pipeline consisting of random crop-resize and slight rotation of the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup image hight and width\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "\n",
    "SIZE = 64\n",
    "\n",
    "def threshold_image(img):\n",
    "    '''\n",
    "    Helper function for thresholding the images\n",
    "    '''\n",
    "    gray = PIL.Image.fromarray(np.uint8(img), 'L')\n",
    "    ret,th = cv.threshold(np.array(gray),0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    return th\n",
    "\n",
    "def train_transforms(p=.5):\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        albu.RandomSizedCrop(min_max_height=(int(SIZE // 1.1), SIZE), height = SIZE, width = SIZE, p=p),\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.Rotate(limit=3, p=p),\n",
    "    ], p=1.0)\n",
    "\n",
    "def valid_transforms():\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.CenterCrop(height = 128, width = 128),\n",
    "        albu.Resize(height = SIZE, width = SIZE)\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a custom pytorch dataset, which will produce images and corresponding labels out of the traing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions to retrieve the images from the dataset in training and validation modes\n",
    "'''\n",
    "\n",
    "def get_image(idx, labels):\n",
    "    '''\n",
    "    Helper function to get the image and label from the training set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = labels.iloc[idx].image_id\n",
    "    filename = PREPROCESSED_PATH + str(image_id) + '.png' \n",
    "    # get the image by id\n",
    "    img = np.asarray(PIL.Image.open(filename))\n",
    "    ret,img = cv.threshold(img,0,255,cv.THRESH_OTSU)\n",
    "    img = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n",
    "    img = 255 - img\n",
    "    # get the labels\n",
    "    row = labels[labels.image_id == image_id]\n",
    "    \n",
    "    # return labels as tuple\n",
    "    labels = row['grapheme_root'].values[0], \\\n",
    "    row['vowel_diacritic'].values[0], \\\n",
    "    row['consonant_diacritic'].values[0]\n",
    "    \n",
    "    return img, labels\n",
    "\n",
    "def get_validation(idx, labels):\n",
    "    '''\n",
    "    Helper function to get the validation image and image_id from the test set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = labels.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    filename = PREPROCESSED_PATH + str(image_id) + '.png' \n",
    "    # get the image by id\n",
    "    img = np.asarray(PIL.Image.open(filename))\n",
    "    ret,img = cv.threshold(img,0,255,cv.THRESH_OTSU)\n",
    "    img = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n",
    "    img = 255 - img\n",
    "    return img, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDataset(Dataset):\n",
    "    '''\n",
    "    Create a custom Bengali images dataset\n",
    "    '''\n",
    "    def __init__(self, transforms, df_labels = None, validation = False):\n",
    "        '''\n",
    "        Init function\n",
    "        INPUT:\n",
    "            df_images - dataframe with the images\n",
    "            transforms - data transforms\n",
    "            df_labels - datafrane containing the target labels\n",
    "            validation - flag indication if the dataset is for training or for validation\n",
    "        '''\n",
    "        self.df_labels = df_labels\n",
    "        self.transforms = transforms\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.validation:\n",
    "            # get the image\n",
    "            img, label = get_image(idx, self.df_labels)\n",
    "            # transform the image\n",
    "            aug = self.transforms(image = img)\n",
    "            img = TF.to_tensor(img)\n",
    "            img = TF.normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            \n",
    "            return img, label\n",
    "        else:\n",
    "            # get the image\n",
    "            img, image_id = get_validation(idx, self.df_labels)\n",
    "            # transform the image\n",
    "            img = TF.to_tensor(img)\n",
    "            img = TF.normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            return img, image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that everything is correct. Let's try to retrieve couple of images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "train_dataset = BengaliDataset(train_transforms(), train_labels)\n",
    "# create a sample trainloader\n",
    "sample_trainloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample train data\n",
    "for img, labels in sample_trainloader:\n",
    "    \n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i][0].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('./kalpurush.ttf')\n",
    "        grapheme_root = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[i].set_title('{}, {}, {}'.format(grapheme_root, vowel_diacritic, consonant_diacritic), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils import model_zoo\n",
    "__all__ = ['SENet', 'se_resnext50_32x4d']\n",
    "class SEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return module_input * x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for bottlenecks that implements `forward()` method.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = self.se_module(out) + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    Bottleneck for SENet154.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
    "        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n",
    "                               stride=stride, padding=1, groups=groups,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes * 4)\n",
    "        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SEResNetBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n",
    "    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n",
    "    (the latter is used in the torchvision implementation of ResNet).\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEResNetBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n",
    "                               stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n",
    "                               groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SEResNeXtBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None, base_width=4):\n",
    "        super(SEResNeXtBottleneck, self).__init__()\n",
    "        width = math.floor(planes * (base_width / 64)) * groups\n",
    "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n",
    "                               stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SENet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n",
    "                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n",
    "                 downsample_padding=1, num_classes=1000):        \n",
    "        super(SENet, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        if input_3x3:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(64)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn2', nn.BatchNorm2d(64)),\n",
    "                ('relu2', nn.ReLU(inplace=True)),\n",
    "                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn3', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu3', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        else:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n",
    "                                    padding=3, bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        # To preserve compatibility with Caffe weights `ceil_mode=True`\n",
    "        # is used instead of `padding=1`.\n",
    "        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n",
    "                                                    ceil_mode=True)))\n",
    "        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n",
    "        self.layer1 = self._make_layer(\n",
    "            block,\n",
    "            planes=64,\n",
    "            blocks=layers[0],\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=1,\n",
    "            downsample_padding=0\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block,\n",
    "            planes=128,\n",
    "            blocks=layers[1],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block,\n",
    "            planes=256,\n",
    "            blocks=layers[2],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block,\n",
    "            planes=512,\n",
    "            blocks=layers[3],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
    "        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n",
    "                    downsample_kernel_size=1, downsample_padding=0):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=downsample_kernel_size, stride=stride,\n",
    "                          padding=downsample_padding, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n",
    "                            downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups, reduction))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "    \n",
    "def se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n",
    "    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n",
    "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = se_resnext50_32x4d(num_classes=500, pretrained=None)\n",
    "backbone_model.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "#backbone_model.last_linear = nn.Linear(model.last_linear.in_features, 186)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom model for the Bengali images\n",
    "backbone model may be replaced with any other archirtecture :)\n",
    "'''\n",
    "\n",
    "class BengaliModel(nn.Module):\n",
    "    def __init__(self, backbone_model):\n",
    "        super(BengaliModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.backbone_model = backbone_model\n",
    "        self.fc1 = nn.Linear(in_features=500, out_features=168) # grapheme_root\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=11) # vowel_diacritic\n",
    "        self.fc3 = nn.Linear(in_features=500, out_features=7) # consonant_diacritic\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass through the backbone model\n",
    "        y = self.conv(x)\n",
    "        y = self.pool(y)\n",
    "        \n",
    "        y = self.backbone_model(y)\n",
    "        \n",
    "        # multi-output\n",
    "        grapheme_root = self.fc1(y)\n",
    "        vowel_diacritic = self.fc2(y)\n",
    "        consonant_diacritic = self.fc3(y)\n",
    "        \n",
    "        return grapheme_root, vowel_diacritic, consonant_diacritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the final model\n",
    "model = BengaliModel(backbone_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set for the modelling:\n",
    "\n",
    "First, let's start with defining the hyperparameters. In this notebook I won't be actually training the model, that is why the number of epochs is 0. I trained the model on my own machine and will just load the weights here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.2\n",
    "batch_size = 128\n",
    "epochs = 50 # change this value to actually train the model\n",
    "learning_rate = 0.001\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"test_split\", test_split)\n",
    "run.log_hyperparameter(\"batch_size\", batch_size)\n",
    "run.log_hyperparameter(\"epochs\", epochs)\n",
    "run.log_hyperparameter(\"learning_rate\", learning_rate)\n",
    "run.log_hyperparameter(\"image_size\", SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the dataset and samplers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "\n",
    "# split the dataset into test and train\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "testloader = DataLoader(train_dataset, batch_size=32, sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizer, only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"optimizer\", \"Adam\")\n",
    "run.log_hyperparameter(\"loss\", \"CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup training device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the logging. I will write the log into pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy'\n",
    "                                      ,'Test loss', 'Test accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = torch.load('resnet50_cutmix_64_transforms_stopped_50.pth', map_location=lambda storage, loc: storage)\n",
    "#model.load_state_dict(state[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to the training device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, learning_rate, epochs):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // (epochs * 0.5))) * (0.1 ** (epoch // (epochs * 0.75)))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "def get_learning_rate(optimizer):\n",
    "    lr = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr += [param_group['lr']]\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\lexie\\lib\\site-packages\\ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10590c24e8ab4817b6a28c729d747793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_accuracy(ps, labels):\n",
    "    '''\n",
    "    Helper function to calculate the accuracy given the labels and the output of the model\n",
    "    '''\n",
    "    ps = torch.exp(ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy = torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    return accuracy\n",
    "\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    train_accuracy = 0\n",
    "    top3_train_accuracy = 0 \n",
    "    for inputs, labels in tqdm_notebook(trainloader):\n",
    "        steps += 1\n",
    "        # move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + \\\n",
    "        criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # get the average accuracy\n",
    "        train_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch, learning_rate, epochs)\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    model.eval()\n",
    "    # run validation on the test set\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "            \n",
    "            grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "            batch_loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "            test_loss += batch_loss.item()\n",
    "            \n",
    "            scheduler.step(test_loss)\n",
    "\n",
    "            # Calculate test top-1 accuracy\n",
    "            test_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "    \n",
    "    # print out the training stats\n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Time per epoch: {time_elapsed:.4f}.. \"\n",
    "          f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n",
    "          f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n",
    "          f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n",
    "          f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n",
    "          f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \")\n",
    "    \n",
    "    filename = 'se_resnext50_'+ str(epoch+1) + '.pth'\n",
    "    checkpoint = {'state_dict': model.state_dict()}\n",
    "    torch.save(checkpoint, filename)\n",
    "    \n",
    "    run.log_observation(\"time_per_epoch\", time_elapsed)\n",
    "    run.log_observation(\"time_per_step\", time_elapsed/len(trainloader))\n",
    "    run.log_observation(\"train_loss\", running_loss/len(trainloader))\n",
    "    run.log_observation(\"test_loss\", test_loss/len(testloader))\n",
    "    run.log_observation(\"train_accuracy\", train_accuracy/len(trainloader))\n",
    "    run.log_observation(\"test_accuracy\", test_accuracy/len(testloader))\n",
    "    run.log_observation(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # write to the training log\n",
    "    train_stats = train_stats.append({'Epoch': epoch, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader),\n",
    "                                      'Train accuracy': train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader),\n",
    "                                      'Test accuracy': test_accuracy/len(testloader)}, ignore_index=True)\n",
    "\n",
    "    running_loss = 0\n",
    "    steps = 0\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'se_resnext50_'+ str(epochs) + '.pth'\n",
    "\n",
    "checkpoint = {'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.to_csv('se_resnext50_train_stats_lr_{}.csv'.format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('model', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('train_stats', train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(train_stats['Train loss'], label='train')\n",
    "plt.plot(train_stats['Test loss'], label='test')\n",
    "plt.title('Loss over epoch')\n",
    "plt.legend()\n",
    "run.log_image(\"loss\", plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy\n",
    "plt.plot(train_stats['Train accuracy'], label='train')\n",
    "plt.plot(train_stats['Test accuracy'], label='test')\n",
    "plt.title('Accuracy over epoch')\n",
    "plt.legend()\n",
    "run.log_image(\"accuracy\", plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize some sample predictions from the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "model.eval()\n",
    "for img, labels in testloader:\n",
    "    img, labels = img.to(device), [label.to(device) for label in labels]\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(img)\n",
    "    \n",
    "    img = img.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # visualize the inputs\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(10,15))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[0].imshow(TF.to_pil_image(img[i].reshape(HEIGHT, WIDTH)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('../input/bengaliaiutils/kalpurush.ttf')\n",
    "        grapheme_root_str = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic_str = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic_str = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[0].set_title('{}, {}, {}'.format(grapheme_root_str, vowel_diacritic_str, consonant_diacritic_str), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "        \n",
    "        # analyze grapheme root prediction\n",
    "        ps_root = F.softmax(grapheme_root[i])\n",
    "        top10_p, top10_class = ps_root.topk(10, dim=0)\n",
    "        \n",
    "        top10_p = top10_p.detach().numpy()\n",
    "        top10_class = top10_class.detach().numpy()\n",
    "        \n",
    "        axs[1].bar(range(len(top10_p)), top10_p)\n",
    "        axs[1].set_xticks(range(len(top10_p)))\n",
    "        axs[1].set_xticklabels(top10_class)\n",
    "        axs[1].set_title('grapheme_root: {}'.format(labels[0][i]))\n",
    "        \n",
    "        # analyze vowel prediction\n",
    "        ps_vowel = F.softmax(vowel_diacritic[i])\n",
    "        top11_p, top11_class = ps_vowel.topk(11, dim=0)\n",
    "        \n",
    "        top11_p = top11_p.detach().numpy()\n",
    "        top11_class = top11_class.detach().numpy()\n",
    "        \n",
    "        axs[2].bar(range(len(top11_p)), top11_p)\n",
    "        axs[2].set_xticks(range(len(top11_p)))\n",
    "        axs[2].set_xticklabels(top11_class)\n",
    "        axs[2].set_title('vowel_diacritic: {}'.format(labels[1][i]))\n",
    "        \n",
    "        # analyze consonant prediction\n",
    "        ps_cons = F.softmax(consonant_diacritic[i])\n",
    "        top7_p, top7_class = ps_cons.topk(7, dim=0)\n",
    "        \n",
    "        top7_p = top7_p.detach().numpy()\n",
    "        top7_class = top7_class.detach().numpy()\n",
    "        \n",
    "        axs[3].bar(range(len(top7_p)), top7_p)\n",
    "        axs[3].set_xticks(range(len(top7_p)))\n",
    "        axs[3].set_xticklabels(top7_class)\n",
    "        axs[3].set_title('consonant_diacritic: {}'.format(labels[2][i]))\n",
    "        \n",
    "        plt.show()\n",
    "        break;\n",
    "        \n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "test_dataset = BengaliDataset(test, valid_transforms(), test_labels, validation = True)\n",
    "sample_validloader = DataLoader(test_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the images from validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "for img, image_ids in sample_validloader:\n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        axs[i].set_title(image_ids[i])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_label(ps):\n",
    "    '''\n",
    "    Helper function to get the predicted label given the probabilities from the model output\n",
    "    '''\n",
    "    ps = F.softmax(ps)[0]\n",
    "    top_p, top_class = ps.topk(1, dim=0)\n",
    "        \n",
    "    top_p = top_p.detach().numpy()\n",
    "    top_class = top_class.detach().numpy()\n",
    "    \n",
    "    return top_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission\n",
    "# initialize the dataframe\n",
    "submission = pd.DataFrame(columns=['row_id', 'target'])\n",
    "\n",
    "for imgs, image_ids in validloader:\n",
    "    img = imgs[0]\n",
    "    image_id = image_ids[0]\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    # forward pass to get the output\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(imgs)\n",
    "    \n",
    "    imgs = imgs.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # get the predicted labels\n",
    "    grapheme_root_label = get_predicted_label(grapheme_root)\n",
    "    vowel_diacritic_label = get_predicted_label(vowel_diacritic)\n",
    "    consonant_diacritic_label = get_predicted_label(consonant_diacritic)\n",
    "    \n",
    "    # add the results to the dataframe\n",
    "    submission = submission.append({'row_id':str(image_id)+'_grapheme_root', 'target':grapheme_root_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_vowel_diacritic', 'target':vowel_diacritic_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_consonant_diacritic', 'target':consonant_diacritic_label}, \n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook I created and trained a sample model. This code can't be used for the actual predcitions for the competition. It requires a lot of optiomization, but you can use it as a sample for learning purposes.\n",
    "\n",
    "## References\n",
    "1. [EfficientNet paper](https://arxiv.org/pdf/1905.11946.pdf)\n",
    "2. [efficientnet-pytorch pacckage](https://pypi.org/project/efficientnet-pytorch/)\n",
    "3. [My EDA notebook for Bengali.AI](https://www.kaggle.com/aleksandradeis/bengali-ai-eda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

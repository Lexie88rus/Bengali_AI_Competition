{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport time\n\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensor\nimport PIL\nimport cv2 as cv\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms.functional as TF\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import SubsetRandomSampler\nfrom torch.optim import Adam,lr_scheduler\n\nfrom tqdm import tqdm_notebook, tqdm","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![image](https://github.com/Lexie88rus/Bengali_AI_Competition/raw/master/assets/samples.png)"},{"metadata":{},"cell_type":"markdown","source":"# Bengali.AI EfficientNet Starter Notebook"},{"metadata":{},"cell_type":"markdown","source":"Bengali.AI is a wonderful competition, especially for the beginners in deep learning. I created this simple pytorch notebook to demonstrate some code for Bengali hadwrittem symbols classification.\n\nI tried to add some data visualization tips throughout this notebook, which help to check the inputs and outputs of the model.\n\nUnfortunately, this code can't be used to make submissions for this competition on Kaggle, but it still can serve as a starting point to develop your own models for Bengali handwriting classification."},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{},"cell_type":"markdown","source":"Specify the path to data and load the csv files:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# setup the input data folder\nDATA_PATH = '../input/bengaliai-cv19/'\n\n# load the dataframes with labels\ntrain_labels = pd.read_csv(DATA_PATH + 'train.csv')\ntest_labels = pd.read_csv(DATA_PATH + 'test.csv')\nclass_map = pd.read_csv(DATA_PATH + 'class_map.csv')\nsample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the test and train sets:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def load_images():\n    '''\n    Helper function to load all train and test images\n    '''\n    train_list = []\n    for i in range(0,4):\n        train_list.append(pd.read_parquet(DATA_PATH + 'train_image_data_{}.parquet'.format(i)))\n    train = pd.concat(train_list, ignore_index=True)\n    \n    test_list = []\n    for i in range(0,4):\n        test_list.append(pd.read_parquet(DATA_PATH + 'test_image_data_{}.parquet'.format(i)))\n    test = pd.concat(test_list, ignore_index=True)\n    \n    return train, test","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = load_images()","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Preprocessing and Data Augmentation"},{"metadata":{},"cell_type":"markdown","source":"Preprocessing and data augmentation are exteremely important for the training of deep learning models. I use the adaptive thresholding to binarize the input images and a simple data augmentation pipeline consisting of random crop-resize and slight rotation of the input images."},{"metadata":{"trusted":true},"cell_type":"code","source":"# setup image hight and width\nHEIGHT = 137\nWIDTH = 236\n\nSIZE = 32\n\ndef threshold_image(img):\n    '''\n    Helper function for thresholding the images\n    '''\n    gray = PIL.Image.fromarray(np.uint8(img), 'L')\n    ret,th = cv.threshold(np.array(gray),0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n    return th\n\ndef train_transforms(p=.5):\n    '''\n    Function returns the training pipeline of augmentations\n    '''\n    return albu.Compose([\n        # compose the random cropping and random rotation\n        albu.CenterCrop(height = 128, width = 128),\n        #albu.Rotate(limit=5, p=p),\n        albu.Resize(height = SIZE, width = SIZE)\n    ], p=1.0)\n\ndef valid_transforms():\n    '''\n    Function returns the training pipeline of augmentations\n    '''\n    return albu.Compose([\n        # compose the random cropping and random rotation\n        albu.CenterCrop(height = 128, width = 128),\n        albu.Resize(height = SIZE, width = SIZE)\n    ], p=1.0)","execution_count":72,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define the Dataset"},{"metadata":{},"cell_type":"markdown","source":"The next step is to create a custom pytorch dataset, which will produce images and corresponding labels out of the traing dataset:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"'''\nHelper functions to retrieve the images from the dataset in training and validation modes\n'''\n\ndef get_image(idx, df, labels):\n    '''\n    Helper function to get the image and label from the training set\n    '''\n    # get the image id by idx\n    image_id = df.iloc[idx].image_id\n    # get the image by id\n    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n    # get the labels\n    row = labels[labels.image_id == image_id]\n    \n    # return labels as tuple\n    labels = row['grapheme_root'].values[0], \\\n    row['vowel_diacritic'].values[0], \\\n    row['consonant_diacritic'].values[0]\n    \n    return img, labels\n\ndef get_validation(idx, df):\n    '''\n    Helper function to get the validation image and image_id from the test set\n    '''\n    # get the image id by idx\n    image_id = df.iloc[idx].image_id\n    # get the image by id\n    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n    return img, image_id","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BengaliDataset(Dataset):\n    '''\n    Create a custom Bengali images dataset\n    '''\n    def __init__(self, df_images, transforms, df_labels = None, validation = False):\n        '''\n        Init function\n        INPUT:\n            df_images - dataframe with the images\n            transforms - data transforms\n            df_labels - datafrane containing the target labels\n            validation - flag indication if the dataset is for training or for validation\n        '''\n        self.df_images = df_images\n        self.df_labels = df_labels\n        self.transforms = transforms\n        self.validation = validation\n\n    def __len__(self):\n        return len(self.df_images)\n\n    def __getitem__(self, idx):\n        if not self.validation:\n            # get the image\n            img, label = get_image(idx, self.df_images, self.df_labels)\n            # threshold the image\n            img = threshold_image(img)\n            # transform the image\n            #img = img.astype(np.uint8)\n            aug = self.transforms(image = img)\n            return TF.to_tensor(aug['image']), label\n        else:\n            # get the image\n            img, image_id = get_validation(idx, self.df_images)\n            # threshold the image\n            img = threshold_image(img)\n            # transform the image\n            #img = img.astype(np.uint8)\n            aug = self.transforms(image = img)\n            # return transformed image and corresponding image_id (instead of label) to create submission\n            return TF.to_tensor(aug['image']), image_id","execution_count":74,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's check that everything is correct. Let's try to retrieve couple of images from the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize train dataset\ntrain_dataset = BengaliDataset(train, train_transforms(), train_labels)\n# create a sample trainloader\nsample_trainloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0)","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot sample train data\nfor img, labels in sample_trainloader:\n    \n    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n    for i in range(0, img.shape[0]):\n        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n        \n        prop = FontProperties()\n        prop.set_file('../input/bengaliaiutils/kalpurush.ttf')\n        grapheme_root = class_map[(class_map.component_type == 'grapheme_root') \\\n                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n        \n        vowel_diacritic = class_map[(class_map.component_type == 'vowel_diacritic') \\\n                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n        \n        consonant_diacritic = class_map[(class_map.component_type == 'consonant_diacritic') \\\n                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n        \n        axs[i].set_title('{}, {}, {}'.format(grapheme_root, vowel_diacritic, consonant_diacritic), \n                         fontproperties=prop, fontsize=20)\n    break;","execution_count":76,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x720 with 5 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Define the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# implement mish activation function\ndef f_mish(input):\n    '''\n    Applies the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n    '''\n    return input * torch.tanh(F.softplus(input))\n\n# implement class wrapper for mish activation function\nclass mish(nn.Module):\n    '''\n    Applies the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n\n    Examples:\n        >>> m = mish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    '''\n    def __init__(self):\n        '''\n        Init method.\n        '''\n        super().__init__()\n\n    def forward(self, input):\n        '''\n        Forward pass of the function.\n        '''\n        return f_mish(input)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implement swish activation function\ndef f_swish(input):\n    '''\n    Applies the swish function element-wise:\n    swish(x) = x * sigmoid(x)\n    '''\n    return input * torch.sigmoid(input)\n\n# implement class wrapper for swish activation function\nclass swish(nn.Module):\n    '''\n    Applies the swish function element-wise:\n    swish(x) = x * sigmoid(x)\n\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n\n    Examples:\n        >>> m = swish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    '''\n    def __init__(self):\n        '''\n        Init method.\n        '''\n        super().__init__()\n\n    def forward(self, input):\n        '''\n        Forward pass of the function.\n        '''\n        return f_swish(input)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearBottleNeck(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride, t=6, class_num=10, activation = 'relu'):\n        super().__init__()\n        \n        if activation == 'relu':\n            f_activation = nn.ReLU6(inplace=True)\n            \n        if activation == 'swish':\n            f_activation = swish()\n            \n        if activation == 'mish':\n            f_activation = mish()\n\n        self.residual = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels * t, 1),\n            nn.BatchNorm2d(in_channels * t),\n            f_activation,\n\n            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n            nn.BatchNorm2d(in_channels * t),\n            f_activation,\n\n            nn.Conv2d(in_channels * t, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n\n        self.stride = stride\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n    \n    def forward(self, x):\n\n        residual = self.residual(x)\n\n        if self.stride == 1 and self.in_channels == self.out_channels:\n            residual += x\n        \n        return residual\n    \nclass MobileNetV2(nn.Module):\n\n    def __init__(self, class_num=10, activation = 'relu'):\n        super().__init__()\n        \n        if activation == 'relu':\n            f_activation = nn.ReLU6(inplace=True)\n            \n        if activation == 'swish':\n            f_activation = swish()\n            \n        if activation == 'mish':\n            f_activation = mish()\n\n        self.pre = nn.Sequential(\n            nn.Conv2d(1, 32, 1, padding=1),\n            nn.BatchNorm2d(32),\n            f_activation\n        )\n\n        self.stage1 = LinearBottleNeck(32, 16, 1, 1, activation = activation)\n        self.stage2 = self._make_stage(2, 16, 24, 2, 6, activation = activation)\n        self.stage3 = self._make_stage(3, 24, 32, 2, 6, activation = activation)\n        self.stage4 = self._make_stage(4, 32, 64, 2, 6, activation = activation)\n        self.stage5 = self._make_stage(3, 64, 96, 1, 6, activation = activation)\n        self.stage6 = self._make_stage(3, 96, 160, 1, 6, activation = activation)\n        self.stage7 = LinearBottleNeck(160, 320, 1, 6, activation = activation)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(320, 1280, 1),\n            nn.BatchNorm2d(1280),\n            f_activation\n        )\n\n        self.conv2 = nn.Conv2d(1280, class_num, 1)\n        \n    def forward(self, x):\n        x = self.pre(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.stage5(x)\n        x = self.stage6(x)\n        x = self.stage7(x)\n        x = self.conv1(x)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = self.conv2(x)\n        x = x.view(x.size(0), -1)\n\n        return x\n    \n    def _make_stage(self, repeat, in_channels, out_channels, stride, t, activation = 'relu'):\n\n        layers = []\n        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t, activation = activation))\n        \n        while repeat - 1:\n            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t, activation = activation))\n            repeat -= 1\n        \n        return nn.Sequential(*layers)\n\ndef mobilenetv2(activation = 'relu'):\n    return MobileNetV2(class_num = 1000, activation = activation)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef channel_split(x, split):\n    \"\"\"split a tensor into two pieces along channel dimension\n    Args:\n        x: input tensor\n        split:(int) channel size for each pieces\n    \"\"\"\n    assert x.size(1) == split * 2\n    return torch.split(x, split, dim=1)\n    \ndef channel_shuffle(x, groups):\n    \"\"\"channel shuffle operation\n    Args:\n        x: input tensor\n        groups: input branch number\n    \"\"\"\n\n    batch_size, channels, height, width = x.size()\n    channels_per_group = int(channels / groups)\n\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    x = x.transpose(1, 2).contiguous()\n    x = x.view(batch_size, -1, height, width)\n\n    return x\n\nclass ShuffleUnit(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        self.stride = stride\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        if stride != 1 or in_channels != out_channels:\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, 1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n                nn.BatchNorm2d(in_channels),\n                nn.Conv2d(in_channels, int(out_channels / 2), 1),\n                nn.BatchNorm2d(int(out_channels / 2)),\n                nn.ReLU(inplace=True)\n            )\n\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n                nn.BatchNorm2d(in_channels),\n                nn.Conv2d(in_channels, int(out_channels / 2), 1),\n                nn.BatchNorm2d(int(out_channels / 2)),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.shortcut = nn.Sequential()\n\n            in_channels = int(in_channels / 2)\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, 1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n                nn.BatchNorm2d(in_channels),\n                nn.Conv2d(in_channels, in_channels, 1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(inplace=True) \n            )\n\n    \n    def forward(self, x):\n\n        if self.stride == 1 and self.out_channels == self.in_channels:\n            shortcut, residual = channel_split(x, int(self.in_channels / 2))\n        else:\n            shortcut = x\n            residual = x\n        \n        shortcut = self.shortcut(shortcut)\n        residual = self.residual(residual)\n        x = torch.cat([shortcut, residual], dim=1)\n        x = channel_shuffle(x, 2)\n        \n        return x\n\nclass ShuffleNetV2(nn.Module):\n\n    def __init__(self, ratio=0.5, class_num=1000):\n        super().__init__()\n        if ratio == 0.5:\n            out_channels = [48, 96, 192, 1024]\n        elif ratio == 1:\n            out_channels = [116, 232, 464, 1024]\n        elif ratio == 1.5:\n            out_channels = [176, 352, 704, 1024]\n        elif ratio == 2:\n            out_channels = [244, 488, 976, 2048]\n        else:\n            ValueError('unsupported ratio number')\n        \n        self.pre = nn.Sequential(\n            nn.Conv2d(1, 24, 3, padding=1),\n            nn.BatchNorm2d(24)\n        )\n\n        self.stage2 = self._make_stage(24, out_channels[0], 3)\n        self.stage3 = self._make_stage(out_channels[0], out_channels[1], 7)\n        self.stage4 = self._make_stage(out_channels[1], out_channels[2], 3)\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(out_channels[2], out_channels[3], 1),\n            nn.BatchNorm2d(out_channels[3]),\n            nn.ReLU(inplace=True)\n        )\n\n        self.fc = nn.Linear(out_channels[3], class_num)\n\n    def forward(self, x):\n        x = self.pre(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.conv5(x)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n    def _make_stage(self, in_channels, out_channels, repeat):\n        layers = []\n        layers.append(ShuffleUnit(in_channels, out_channels, 2))\n\n        while repeat:\n            layers.append(ShuffleUnit(out_channels, out_channels, 1))\n            repeat -= 1\n        \n        return nn.Sequential(*layers)\n\ndef shufflenetv2():\n    return ShuffleNetV2()","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"backbone_model = shufflenetv2()","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nCustom model for the Bengali images\nbackbone model may be replaced with any other archirtecture :)\n'''\n\nclass BengaliModel(nn.Module):\n    def __init__(self, backbone_model):\n        super(BengaliModel, self).__init__()\n        #self.conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)\n        self.backbone_model = backbone_model\n        self.fc1 = nn.Linear(in_features=1000, out_features=168) # grapheme_root\n        self.fc2 = nn.Linear(in_features=1000, out_features=11) # vowel_diacritic\n        self.fc3 = nn.Linear(in_features=1000, out_features=7) # consonant_diacritic\n        \n    def forward(self, x):\n        # pass through the backbone model\n        #y = self.conv(x)\n        y = self.backbone_model(x)\n        \n        # multi-output\n        grapheme_root = self.fc1(y)\n        vowel_diacritic = self.fc2(y)\n        consonant_diacritic = self.fc3(y)\n        \n        return grapheme_root, vowel_diacritic, consonant_diacritic","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize the final model\nmodel = BengaliModel(backbone_model)","execution_count":58,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the Model"},{"metadata":{},"cell_type":"markdown","source":"Now we are all set for the modelling:\n\nFirst, let's start with defining the hyperparameters. In this notebook I won't be actually training the model, that is why the number of epochs is 0. I trained the model on my own machine and will just load the weights here."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_split = 0.2\nbatch_size = 64\nepochs = 1 # change this value to actually train the model\nlearning_rate = 0.001\nnum_workers = 0","execution_count":59,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setup the dataset and samplers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_size = len(train_dataset)\n\n# split the dataset into test and train\nindices = list(range(dataset_size))\nsplit = int(np.floor(test_split * dataset_size))\nnp.random.seed(42)\nnp.random.shuffle(indices)\ntrain_indices, test_indices = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\ntrainloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\ntestloader = DataLoader(train_dataset, batch_size=32, sampler=test_sampler, num_workers=num_workers)","execution_count":60,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optimizer and loss function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set loss function\ncriterion = nn.CrossEntropyLoss()\n\n# set optimizer, only train the classifier parameters, feature parameters are frozen\noptimizer = Adam(model.parameters(), lr=learning_rate)","execution_count":61,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define a training device:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# setup training device\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":62,"outputs":[{"output_type":"execute_result","execution_count":62,"data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Setup the logging. I will write the log into pandas DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy'\n                                      ,'Test loss', 'Test accuracy'])","execution_count":63,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to train! "},{"metadata":{"trusted":true},"cell_type":"code","source":"# move the model to the training device\nmodel = model.to(device)","execution_count":64,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# I'm just loading the weights instead of training\n#state = torch.load('../input/bengaliaiutils/efficientnet_b0_10.pth', map_location=lambda storage, loc: storage)\n#model.load_state_dict(state[\"state_dict\"])","execution_count":65,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training loop:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_accuracy(ps, labels):\n    '''\n    Helper function to calculate the accuracy given the labels and the output of the model\n    '''\n    ps = torch.exp(ps)\n    top_p, top_class = ps.topk(1, dim=1)\n    equals = top_class == labels.view(*top_class.shape)\n    accuracy = torch.mean(equals.type(torch.FloatTensor)).item()\n    return accuracy\n\nsteps = 0\nrunning_loss = 0\nfor epoch in range(epochs):\n    \n    since = time.time()\n    \n    train_accuracy = 0\n    top3_train_accuracy = 0 \n    for inputs, labels in tqdm(trainloader):\n        steps += 1\n        # move input and label tensors to the default device\n        inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n        \n        optimizer.zero_grad()\n        \n        # forward pass\n        grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n        \n        # calculate the loss\n        loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + \\\n        criterion(consonant_diacritic, labels[2])\n        \n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        \n        # get the average accuracy\n        train_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n\n        \n    time_elapsed = time.time() - since\n    \n    test_loss = 0\n    test_accuracy = 0\n    model.eval()\n    # run validation on the test set\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n            \n            grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n            batch_loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + criterion(consonant_diacritic, labels[2])\n        \n            test_loss += batch_loss.item()\n\n            # Calculate test top-1 accuracy\n            test_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n    \n    # print out the training stats\n    print(f\"Epoch {epoch+1}/{epochs}.. \"\n          f\"Time per epoch: {time_elapsed:.4f}.. \"\n          f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n          f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n          f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n          f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n          f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \")\n\n    # write to the training log\n    train_stats = train_stats.append({'Epoch': epoch, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader),\n                                      'Train accuracy': train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader),\n                                      'Test accuracy': test_accuracy/len(testloader)}, ignore_index=True)\n\n    running_loss = 0\n    steps = 0\n    model.train()","execution_count":66,"outputs":[{"output_type":"stream","text":"\n\n\n\n\n\n  0%|          | 0/2511 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 1/2511 [00:06<4:25:55,  6.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 2/2511 [00:12<4:22:02,  6.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 3/2511 [00:18<4:19:45,  6.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 4/2511 [00:24<4:16:02,  6.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 5/2511 [00:30<4:15:35,  6.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 6/2511 [00:36<4:12:56,  6.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 7/2511 [00:42<4:13:45,  6.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 8/2511 [00:49<4:20:07,  6.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 9/2511 [00:55<4:20:00,  6.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 10/2511 [01:01<4:18:03,  6.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 11/2511 [01:07<4:14:10,  6.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  0%|          | 12/2511 [01:13<4:13:17,  6.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  1%|          | 13/2511 [01:19<4:13:20,  6.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n  1%|          | 14/2511 [01:25<4:12:33,  6.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A","name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-b37475e89414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtop3_train_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# move input and label tensors to the default device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1091\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-52-c092e8a7fd23>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# get the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;31m# threshold the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-31a163ee804c>\u001b[0m in \u001b[0;36mget_image\u001b[0;34m(idx, df, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# get the image by id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# get the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 raise TypeError(\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_datetimelike_v_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"## Analyze Results"},{"metadata":{},"cell_type":"markdown","source":"Now we can look at the training results:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# I load my training stats from the local machine :)\ntrain_stats = pd.read_csv('../input/bengaliaiutils/efficientnet_b0_train_stats_10.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the loss\nplt.plot(train_stats['Train loss'], label='train')\nplt.plot(train_stats['Test loss'], label='test')\nplt.title('Loss over epoch')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the accuracy\nplt.plot(train_stats['Train accuracy'], label='train')\nplt.plot(train_stats['Test accuracy'], label='test')\nplt.title('Accuracy over epoch')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also visualize some sample predictions from the train set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot sample train data\nmodel.eval()\nfor img, labels in testloader:\n    img, labels = img.to(device), [label.to(device) for label in labels]\n    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(img)\n    \n    img = img.cpu()\n    grapheme_root = grapheme_root.cpu()\n    vowel_diacritic = vowel_diacritic.cpu()\n    consonant_diacritic = consonant_diacritic.cpu()\n    \n    # visualize the inputs\n    fig, axs = plt.subplots(4, 1, figsize=(10,15))\n    for i in range(0, img.shape[0]):\n        axs[0].imshow(TF.to_pil_image(img[i].reshape(HEIGHT, WIDTH)), cmap='gray')\n        \n        prop = FontProperties()\n        prop.set_file('../input/bengaliaiutils/kalpurush.ttf')\n        grapheme_root_str = class_map[(class_map.component_type == 'grapheme_root') \\\n                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n        \n        vowel_diacritic_str = class_map[(class_map.component_type == 'vowel_diacritic') \\\n                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n        \n        consonant_diacritic_str = class_map[(class_map.component_type == 'consonant_diacritic') \\\n                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n        \n        axs[0].set_title('{}, {}, {}'.format(grapheme_root_str, vowel_diacritic_str, consonant_diacritic_str), \n                         fontproperties=prop, fontsize=20)\n        \n        # analyze grapheme root prediction\n        ps_root = F.softmax(grapheme_root[i])\n        top10_p, top10_class = ps_root.topk(10, dim=0)\n        \n        top10_p = top10_p.detach().numpy()\n        top10_class = top10_class.detach().numpy()\n        \n        axs[1].bar(range(len(top10_p)), top10_p)\n        axs[1].set_xticks(range(len(top10_p)))\n        axs[1].set_xticklabels(top10_class)\n        axs[1].set_title('grapheme_root: {}'.format(labels[0][i]))\n        \n        # analyze vowel prediction\n        ps_vowel = F.softmax(vowel_diacritic[i])\n        top11_p, top11_class = ps_vowel.topk(11, dim=0)\n        \n        top11_p = top11_p.detach().numpy()\n        top11_class = top11_class.detach().numpy()\n        \n        axs[2].bar(range(len(top11_p)), top11_p)\n        axs[2].set_xticks(range(len(top11_p)))\n        axs[2].set_xticklabels(top11_class)\n        axs[2].set_title('vowel_diacritic: {}'.format(labels[1][i]))\n        \n        # analyze consonant prediction\n        ps_cons = F.softmax(consonant_diacritic[i])\n        top7_p, top7_class = ps_cons.topk(7, dim=0)\n        \n        top7_p = top7_p.detach().numpy()\n        top7_class = top7_class.detach().numpy()\n        \n        axs[3].bar(range(len(top7_p)), top7_p)\n        axs[3].set_xticks(range(len(top7_p)))\n        axs[3].set_xticklabels(top7_class)\n        axs[3].set_title('consonant_diacritic: {}'.format(labels[2][i]))\n        \n        plt.show()\n        break;\n        \n    break;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Submission"},{"metadata":{},"cell_type":"markdown","source":"The last step is to create a submission:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize train dataset\ntest_dataset = BengaliDataset(test, valid_transforms(), test_labels, validation = True)\nsample_validloader = DataLoader(test_dataset, batch_size=5, shuffle=True, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at the images from validation set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot sample train data\nfor img, image_ids in sample_validloader:\n    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n    for i in range(0, img.shape[0]):\n        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n        axs[i].set_title(image_ids[i])\n    break;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the submission:"},{"metadata":{"trusted":true},"cell_type":"code","source":"validloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predicted_label(ps):\n    '''\n    Helper function to get the predicted label given the probabilities from the model output\n    '''\n    ps = F.softmax(ps)[0]\n    top_p, top_class = ps.topk(1, dim=0)\n        \n    top_p = top_p.detach().numpy()\n    top_class = top_class.detach().numpy()\n    \n    return top_class[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the submission\n# initialize the dataframe\nsubmission = pd.DataFrame(columns=['row_id', 'target'])\n\nfor imgs, image_ids in validloader:\n    img = imgs[0]\n    image_id = image_ids[0]\n    \n    imgs = imgs.to(device)\n    \n    # forward pass to get the output\n    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(imgs)\n    \n    imgs = imgs.cpu()\n    grapheme_root = grapheme_root.cpu()\n    vowel_diacritic = vowel_diacritic.cpu()\n    consonant_diacritic = consonant_diacritic.cpu()\n    \n    # get the predicted labels\n    grapheme_root_label = get_predicted_label(grapheme_root)\n    vowel_diacritic_label = get_predicted_label(vowel_diacritic)\n    consonant_diacritic_label = get_predicted_label(consonant_diacritic)\n    \n    # add the results to the dataframe\n    submission = submission.append({'row_id':str(image_id)+'_grapheme_root', 'target':grapheme_root_label}, \n                                   ignore_index=True)\n    submission = submission.append({'row_id':str(image_id)+'_vowel_diacritic', 'target':vowel_diacritic_label}, \n                                   ignore_index=True)\n    submission = submission.append({'row_id':str(image_id)+'_consonant_diacritic', 'target':consonant_diacritic_label}, \n                                   ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at the submission file:"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook I created and trained a sample model. This code can't be used for the actual predcitions for the competition. It requires a lot of optiomization, but you can use it as a sample for learning purposes.\n\n## References\n1. [EfficientNet paper](https://arxiv.org/pdf/1905.11946.pdf)\n2. [efficientnet-pytorch pacckage](https://pypi.org/project/efficientnet-pytorch/)\n3. [My EDA notebook for Bengali.AI](https://www.kaggle.com/aleksandradeis/bengali-ai-eda)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}
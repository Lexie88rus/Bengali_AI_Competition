{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport torchvision\n\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom albumentations import Compose\nfrom albumentations.pytorch import ToTensorV2\n\nimport PIL\n\nimport cv2 as cv\n\nINPUT_PATH = '/kaggle/input/bengaliai-cv19'","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ======================\n# Params\nBATCH_SIZE = 32\nN_WORKERS = 4\nN_EPOCHS = 5\n\nHEIGHT = 137\nWIDTH = 236\nTARGET_SIZE = 256\n\n# My weights dataset for this compeititon; feel free to vote the dataste ;)\n# https://www.kaggle.com/pestipeti/bengali-ai-model-weights\nWEIGHTS_FILE = '/kaggle/input/bengali-ai-model-weights/baseline_weights.pth'","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setup image hight and width\nHEIGHT = 137\nWIDTH = 236\nSIZE = 128\n\ndef bbox(img):\n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n    return rmin, rmax, cmin, cmax\n\ndef crop_resize(img0, size=SIZE, pad=16):\n    #crop a box around pixels large than the threshold \n    #some images contain line at the sides\n    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n    #cropping may cut too much, so we need to add it back\n    xmin = xmin - 13 if (xmin > 13) else 0\n    ymin = ymin - 10 if (ymin > 10) else 0\n    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n    img = img0[ymin:ymax,xmin:xmax]\n    #remove low intensity pixels as noise\n    #img[img < 28] = 0\n    lx, ly = xmax-xmin,ymax-ymin\n    l = max(lx,ly) + pad\n    #make sure that the aspect ratio is kept in rescaling\n    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n    return cv.resize(img,(size,size))\n\ndef threshold_image(img):\n    '''\n    Helper function for thresholding the images\n    '''\n    gray = PIL.Image.fromarray(np.uint8(img), 'L')\n    ret,th = cv.threshold(np.array(gray),0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n    return th\n\nclass BengaliParquetDataset(Dataset):\n\n    def __init__(self, parquet_file, transform=None):\n\n        self.data = pd.read_parquet(parquet_file)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img = self.data.iloc[idx, 1:].values.reshape(HEIGHT, WIDTH)\n        \n        img = 255.0 - img\n        img = (img*(255.0/img.max())).astype(np.uint8)\n        img = crop_resize(img, size=SIZE, pad=16)\n        img = threshold_image(img)\n        img = 255 - img\n\n        image_id = self.data.iloc[idx, 0]\n\n        if self.transform:\n            transformed = self.transform(image=img)\n            img = transformed['image']\n            \n        img = TF.to_tensor(img)\n\n        return {\n            'image_id': image_id,\n            'image': img\n        }","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implement mish activation function\ndef f_mish(input):\n    '''\n    Applies the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n    '''\n    return input * torch.tanh(F.softplus(input))\n\n# implement class wrapper for mish activation function\nclass mish(nn.Module):\n    '''\n    Applies the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n\n    Examples:\n        >>> m = mish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    '''\n    def __init__(self):\n        '''\n        Init method.\n        '''\n        super().__init__()\n\n    def forward(self, input):\n        '''\n        Forward pass of the function.\n        '''\n        return f_mish(input)\n    \n# implement swish activation function\ndef f_swish(input):\n    '''\n    Applies the swish function element-wise:\n    swish(x) = x * sigmoid(x)\n    '''\n    return input * torch.sigmoid(input)\n\n# implement class wrapper for swish activation function\nclass swish(nn.Module):\n    '''\n    Applies the swish function element-wise:\n    swish(x) = x * sigmoid(x)\n\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n\n    Examples:\n        >>> m = swish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    '''\n    def __init__(self):\n        '''\n        Init method.\n        '''\n        super().__init__()\n\n    def forward(self, input):\n        '''\n        Forward pass of the function.\n        '''\n        return f_swish(input)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearBottleNeck(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride, t=6, class_num=10, activation = 'relu'):\n        super().__init__()\n        \n        if activation == 'relu':\n            f_activation = nn.ReLU6(inplace=True)\n            \n        if activation == 'swish':\n            f_activation = swish()\n            \n        if activation == 'mish':\n            f_activation = mish()\n\n        self.residual = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels * t, 1),\n            nn.BatchNorm2d(in_channels * t),\n            f_activation,\n\n            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n            nn.BatchNorm2d(in_channels * t),\n            f_activation,\n\n            nn.Conv2d(in_channels * t, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n\n        self.stride = stride\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n    \n    def forward(self, x):\n\n        residual = self.residual(x)\n\n        if self.stride == 1 and self.in_channels == self.out_channels:\n            residual += x\n        \n        return residual\n    \nclass MobileNetV2(nn.Module):\n\n    def __init__(self, class_num=10, activation = 'relu'):\n        super().__init__()\n        \n        if activation == 'relu':\n            f_activation = nn.ReLU6(inplace=True)\n            \n        if activation == 'swish':\n            f_activation = swish()\n            \n        if activation == 'mish':\n            f_activation = mish()\n\n        self.pre = nn.Sequential(\n            nn.Conv2d(1, 32, 1, padding=1),\n            nn.BatchNorm2d(32),\n            f_activation\n        )\n\n        self.stage1 = LinearBottleNeck(32, 16, 1, 1, activation = activation)\n        self.stage2 = self._make_stage(2, 16, 24, 2, 6, activation = activation)\n        self.stage3 = self._make_stage(3, 24, 32, 2, 6, activation = activation)\n        self.stage4 = self._make_stage(4, 32, 64, 2, 6, activation = activation)\n        self.stage5 = self._make_stage(3, 64, 96, 1, 6, activation = activation)\n        self.stage6 = self._make_stage(3, 96, 160, 1, 6, activation = activation)\n        self.stage7 = LinearBottleNeck(160, 320, 1, 6, activation = activation)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(320, 1280, 1),\n            nn.BatchNorm2d(1280),\n            f_activation\n        )\n\n        self.conv2 = nn.Conv2d(1280, class_num, 1)\n        \n    def forward(self, x):\n        x = self.pre(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.stage5(x)\n        x = self.stage6(x)\n        x = self.stage7(x)\n        x = self.conv1(x)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = self.conv2(x)\n        x = x.view(x.size(0), -1)\n\n        return x\n    \n    def _make_stage(self, repeat, in_channels, out_channels, stride, t, activation = 'relu'):\n\n        layers = []\n        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t, activation = activation))\n        \n        while repeat - 1:\n            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t, activation = activation))\n            repeat -= 1\n        \n        return nn.Sequential(*layers)\n\ndef mobilenetv2(activation = 'relu'):\n    return MobileNetV2(class_num = 1000, activation = activation)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"backbone_model = mobilenetv2()","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BengaliModel(nn.Module):\n    def __init__(self, backbone_model):\n        super(BengaliModel, self).__init__()\n        #self.conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)\n        self.backbone_model = backbone_model\n        self.fc1 = nn.Linear(in_features=1000, out_features=168) # grapheme_root\n        self.fc2 = nn.Linear(in_features=1000, out_features=11) # vowel_diacritic\n        self.fc3 = nn.Linear(in_features=1000, out_features=7) # consonant_diacritic\n        \n    def forward(self, x):\n        # pass through the backbone model\n        #y = self.conv(x)\n        y = self.backbone_model(x)\n        \n        # multi-output\n        grapheme_root = self.fc1(y)\n        vowel_diacritic = self.fc2(y)\n        consonant_diacritic = self.fc3(y)\n        \n        return grapheme_root, vowel_diacritic, consonant_diacritic","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BengaliModel(backbone_model)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state = torch.load('../input/bengaliaiutils/mobilenet_v2_15.pth', map_location=lambda storage, loc: storage)\nmodel.load_state_dict(state[\"state_dict\"])","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(INPUT_PATH + '/test.csv')\nsubmission_df = pd.read_csv(INPUT_PATH + '/sample_submission.csv')\n\ndevice = torch.device(\"cuda:0\")\nmodel.to(device)\n\nresults = []","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(4):\n    parq = INPUT_PATH + '/test_image_data_{}.parquet'.format(i)\n    test_dataset = BengaliParquetDataset(\n        parquet_file=parq,\n        transform=None\n    )\n    data_loader_test = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=BATCH_SIZE,\n        num_workers=N_WORKERS,\n        shuffle=False\n    )\n\n    print('Parquet {}'.format(i))\n\n    model.eval()\n\n    tk0 = tqdm(data_loader_test, desc=\"Iteration\")\n\n    for step, batch in enumerate(tk0):\n        inputs = batch[\"image\"]\n        image_ids = batch[\"image_id\"]\n        inputs = inputs.to(device, dtype=torch.float)\n\n        out_graph, out_vowel, out_conso = model(inputs)\n        out_graph = F.softmax(out_graph, dim=1).data.cpu().numpy().argmax(axis=1)\n        out_vowel = F.softmax(out_vowel, dim=1).data.cpu().numpy().argmax(axis=1)\n        out_conso = F.softmax(out_conso, dim=1).data.cpu().numpy().argmax(axis=1)\n\n        for idx, image_id in enumerate(image_ids):\n            results.append(out_conso[idx])\n            results.append(out_graph[idx])\n            results.append(out_vowel[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df['target'] = results\nsubmission_df.to_csv('./submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}
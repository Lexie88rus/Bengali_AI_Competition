{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from efficientnet_pytorch.utils import Conv2dStaticSamePadding, get_model_params\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"BengaliAI\"\n",
    "EXPERIMENT_NAME = \"EffNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VERTA_EMAIL'] = 'astakhova.aleksandra@gmail.com'\n",
    "os.environ['VERTA_DEV_KEY'] = 'd7ee32b5-bbd0-4c4c-a2ec-a070848021be'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "set existing Project: BengaliAI\n",
      "set existing Experiment: EffNet\n",
      "created new ExperimentRun: Run 1745215793920591274464\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "run.log_tag('EffNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the input data folder\n",
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with labels\n",
    "train_labels = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_labels = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "class_map = pd.read_csv(DATA_PATH + 'class_map.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_type</th>\n",
       "      <th>label</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grapheme_root</td>\n",
       "      <td>0</td>\n",
       "      <td>ং</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grapheme_root</td>\n",
       "      <td>1</td>\n",
       "      <td>ঃ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grapheme_root</td>\n",
       "      <td>2</td>\n",
       "      <td>অ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grapheme_root</td>\n",
       "      <td>3</td>\n",
       "      <td>আ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grapheme_root</td>\n",
       "      <td>4</td>\n",
       "      <td>ই</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  component_type  label component\n",
       "0  grapheme_root      0         ং\n",
       "1  grapheme_root      1         ঃ\n",
       "2  grapheme_root      2         অ\n",
       "3  grapheme_root      3         আ\n",
       "4  grapheme_root      4         ই"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    '''\n",
    "    Helper function to load all train and test images\n",
    "    '''\n",
    "    train_list = []\n",
    "    for i in range(0,4):\n",
    "        train_list.append(pd.read_parquet(DATA_PATH + 'train_image_data_{}.parquet'.format(i)))\n",
    "    train = pd.concat(train_list, ignore_index=True)\n",
    "    \n",
    "    test_list = []\n",
    "    for i in range(0,4):\n",
    "        test_list.append(pd.read_parquet(DATA_PATH + 'test_image_data_{}.parquet'.format(i)))\n",
    "    test = pd.concat(test_list, ignore_index=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing and Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup image hight and width\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "SIZE = 128\n",
    "\n",
    "# Source:\n",
    "# https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "\n",
    "def bbox(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def crop_resize(img0, size=SIZE, pad=16):\n",
    "    #crop a box around pixels large than the threshold \n",
    "    #some images contain line at the sides\n",
    "    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n",
    "    #cropping may cut too much, so we need to add it back\n",
    "    xmin = xmin - 13 if (xmin > 13) else 0\n",
    "    ymin = ymin - 10 if (ymin > 10) else 0\n",
    "    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n",
    "    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n",
    "    img = img0[ymin:ymax,xmin:xmax]\n",
    "    #remove low intensity pixels as noise\n",
    "    #img[img < 28] = 0\n",
    "    lx, ly = xmax-xmin,ymax-ymin\n",
    "    l = max(lx,ly) + pad\n",
    "    #make sure that the aspect ratio is kept in rescaling\n",
    "    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n",
    "    return cv.resize(img,(size,size))\n",
    "\n",
    "def threshold_image(img):\n",
    "    '''\n",
    "    Helper function for thresholding the images\n",
    "    '''\n",
    "    gray = PIL.Image.fromarray(np.uint8(img), 'L')\n",
    "    ret,th = cv.threshold(np.array(gray),0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    return th\n",
    "\n",
    "def train_transforms(p=.5):\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.RandomSizedCrop(min_max_height=(int(SIZE // 1.1), SIZE), height = SIZE, width = SIZE, p=p),\n",
    "        albu.Rotate(limit=5, p=p)\n",
    "    ], p=1.0)\n",
    "\n",
    "def valid_transforms():\n",
    "    '''\n",
    "    Function returns the training pipeline of augmentations\n",
    "    '''\n",
    "    return albu.Compose([\n",
    "        # compose the random cropping and random rotation\n",
    "        albu.Resize(height = SIZE, width = SIZE)\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the custom dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(idx, df, labels):\n",
    "    '''\n",
    "    Helper function to get the image and label from the training set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n",
    "    # get the labels\n",
    "    row = labels[labels.image_id == image_id]\n",
    "    labels = row['grapheme_root'].values[0], \\\n",
    "    row['vowel_diacritic'].values[0], \\\n",
    "    row['consonant_diacritic'].values[0]\n",
    "    \n",
    "    return img, labels\n",
    "\n",
    "def get_validation(idx, df):\n",
    "    '''\n",
    "    Helper function to get the validation image and image_id from the test set\n",
    "    '''\n",
    "    # get the image id by idx\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    # get the image by id\n",
    "    img = df[df.image_id == image_id].values[:, 1:].reshape(HEIGHT, WIDTH).astype(float)\n",
    "    return img, image_id\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    '''\n",
    "    Create custom Bengali dataset\n",
    "    '''\n",
    "    def __init__(self, df_images, transforms, df_labels = None, validation = False):\n",
    "        self.df_images = df_images\n",
    "        self.df_labels = df_labels\n",
    "        self.transforms = transforms\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.validation:\n",
    "            img, label = get_image(idx, self.df_images, self.df_labels)\n",
    "            img = 255.0 - img\n",
    "            img = (img*(255.0/img.max())).astype(np.uint8)\n",
    "            img = crop_resize(img, size=SIZE, pad=16)\n",
    "            img = threshold_image(img)\n",
    "            img = 255 - img\n",
    "            aug = self.transforms(image = img)\n",
    "            return TF.to_tensor(aug['image']), label\n",
    "        else:\n",
    "            img, image_id = get_validation(idx, self.df_images)\n",
    "            img = 255.0 - img\n",
    "            img = (img*(255.0/img.max())).astype(np.uint8)\n",
    "            img = crop_resize(img, size=SIZE, pad=16)\n",
    "            img = threshold_image(img)\n",
    "            img = 255 - img\n",
    "            if self.transforms != None:\n",
    "                aug = self.transforms(image = img)\n",
    "                return TF.to_tensor(aug['image']), image_id\n",
    "            else:\n",
    "                return img, image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get some images and labels from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "train_dataset = BengaliDataset(train, train_transforms(), train_labels)\n",
    "sample_trainloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample train data\n",
    "for img, labels in sample_trainloader:\n",
    "    \n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('./kalpurush.ttf')\n",
    "        grapheme_root = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[i].set_title('{}, {}, {}'.format(grapheme_root, vowel_diacritic, consonant_diacritic), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the EfficientNet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "EffNet: AN EFFICIENT STRUCTURE FOR CONVOLUTIONAL NEURAL NETWORKS\n",
    "Implementation in Pytorch of Effnet.\n",
    "https://arxiv.org/abs/1801.06434\n",
    "'''\n",
    "import torch.nn as nn\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n",
    "     \n",
    "class EffNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_classes=1000, include_top=True, weights=None):\n",
    "        super(EffNet, self).__init__()\n",
    "        \n",
    "        self.block1 = self.make_layers(32, 64)\n",
    "        self.block2 = self.make_layers(64, 128)\n",
    "        self.block3 = self.make_layers(128, 256)\n",
    "        self.flatten = Flatten()\n",
    "        self.linear = nn.Linear(65536, nb_classes)\n",
    "        self.include_top = include_top\n",
    "        self.weights = weights\n",
    "\n",
    "    def make_layers(self, ch_in, ch_out):\n",
    "        layers = [\n",
    "            nn.Conv2d(1, ch_in, kernel_size=(1,1), stride=(1,1), bias=False, padding=0, dilation=(1,1)) if ch_in ==32 else nn.Conv2d(ch_in, ch_in, kernel_size=(1,1),stride=(1,1), bias=False, padding=0, dilation=(1,1)) ,\n",
    "            self.make_post(ch_in),\n",
    "            # DepthWiseConvolution2D\n",
    "            nn.Conv2d(ch_in, 1 * ch_in, groups=ch_in, kernel_size=(1, 3),stride=(1,1), padding=(0,1), bias=False, dilation=(1,1)),\n",
    "            self.make_post(ch_in),\n",
    "            nn.MaxPool2d(kernel_size=(2,1), stride=(2,1)),\n",
    "            # DepthWiseConvolution2D\n",
    "            nn.Conv2d(ch_in, 1 * ch_in, groups=ch_in, kernel_size=(3, 1), stride=(1,1), padding=(1,0), bias=False, dilation=(1,1)),\n",
    "            self.make_post(ch_in),\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=(1, 2), stride=(1, 2), bias=False, padding=(0,0), dilation=(1,1)),\n",
    "            self.make_post(ch_out),\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_post(self, ch_in):\n",
    "        layers = [\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.BatchNorm2d(ch_in, momentum=0.99)\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        if self.include_top:\n",
    "            x = self.flatten(x)\n",
    "            x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = EffNet(nb_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliModel(nn.Module):\n",
    "    def __init__(self, backbone_model):\n",
    "        super(BengaliModel, self).__init__()\n",
    "        #self.conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)\n",
    "        self.backbone_model = backbone_model\n",
    "        self.fc1 = nn.Linear(in_features=1000, out_features=168) # grapheme_root\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=11) # vowel_diacritic\n",
    "        self.fc3 = nn.Linear(in_features=1000, out_features=7) # consonant_diacritic\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass through the backbone model\n",
    "        #y = self.conv(x)\n",
    "        y = self.backbone_model(x)\n",
    "        \n",
    "        # multi-output\n",
    "        grapheme_root = self.fc1(y)\n",
    "        vowel_diacritic = self.fc2(y)\n",
    "        consonant_diacritic = self.fc3(y)\n",
    "        \n",
    "        return grapheme_root, vowel_diacritic, consonant_diacritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengaliModel(backbone_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.2\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"test_split\", test_split)\n",
    "run.log_hyperparameter(\"batch_size\", batch_size)\n",
    "run.log_hyperparameter(\"epochs\", epochs)\n",
    "run.log_hyperparameter(\"learning_rate\", learning_rate)\n",
    "run.log_hyperparameter(\"image_size\", SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the samplers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "testloader = DataLoader(train_dataset, batch_size=32, sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizer, only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"optimizer\", \"Adam\")\n",
    "run.log_hyperparameter(\"loss\", \"CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup training device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dataframe to store training statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy'\n",
    "                                      ,'Test loss', 'Test accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(ps, labels):\n",
    "    ps = torch.exp(ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy = torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = torch.load('mobilenet_v2_10.pth', map_location=lambda storage, loc: storage)\n",
    "#model.load_state_dict(state[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\lexie\\lib\\site-packages\\ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adf0b79bd0648429078ef74b4f14d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10.. Time per epoch: 5515.1676.. Average time per step: 2.1964.. Train loss: 1.9329.. Train accuracy: 0.8088.. Test loss: 1.9166.. Test accuracy: 0.8136.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d4aeebd67d43e5a95ea977b533a0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10.. Time per epoch: 5551.3493.. Average time per step: 2.2108.. Train loss: 1.6844.. Train accuracy: 0.8325.. Test loss: 1.8068.. Test accuracy: 0.8243.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede1a461d66f4bd1ac8da74f6d50a5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10.. Time per epoch: 5514.9583.. Average time per step: 2.1963.. Train loss: 1.5013.. Train accuracy: 0.8501.. Test loss: 1.6371.. Test accuracy: 0.8404.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10132ad87024e069469e07d54a259b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10.. Time per epoch: 5954.4050.. Average time per step: 2.3713.. Train loss: 1.3531.. Train accuracy: 0.8632.. Test loss: 1.6230.. Test accuracy: 0.8436.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f63ce1cb774fa88da428d639a8c905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10.. Time per epoch: 5808.3380.. Average time per step: 2.3132.. Train loss: 1.2420.. Train accuracy: 0.8739.. Test loss: 1.6080.. Test accuracy: 0.8497.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7958b87d0841447da86f0daf374ec77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10.. Time per epoch: 5588.3381.. Average time per step: 2.2255.. Train loss: 1.1499.. Train accuracy: 0.8828.. Test loss: 1.6232.. Test accuracy: 0.8480.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15c736e8c724d9f98d48447d5aca9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10.. Time per epoch: 5593.2511.. Average time per step: 2.2275.. Train loss: 1.0731.. Train accuracy: 0.8897.. Test loss: 1.6237.. Test accuracy: 0.8527.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6035989641974b1aa55113363b627044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10.. Time per epoch: 5606.7689.. Average time per step: 2.2329.. Train loss: 1.0126.. Train accuracy: 0.8961.. Test loss: 1.7088.. Test accuracy: 0.8506.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620c8e1a8b5d4b27a391841ff6d9417c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10.. Time per epoch: 5655.8962.. Average time per step: 2.2524.. Train loss: 0.9644.. Train accuracy: 0.9009.. Test loss: 1.7080.. Test accuracy: 0.8528.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8631d0c1294caba5a82cbced655d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10.. Time per epoch: 5625.1172.. Average time per step: 2.2402.. Train loss: 0.9179.. Train accuracy: 0.9051.. Test loss: 1.7690.. Test accuracy: 0.8493.. \n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "running_loss = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    train_accuracy = 0\n",
    "    top3_train_accuracy = 0 \n",
    "    for inputs, labels in tqdm_notebook(trainloader):\n",
    "        steps += 1\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "        \n",
    "        loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + \\\n",
    "        criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        train_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), [label.to(device) for label in labels]\n",
    "            \n",
    "            grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(inputs)\n",
    "            batch_loss = criterion(grapheme_root, labels[0]) + criterion(vowel_diacritic, labels[1]) + criterion(consonant_diacritic, labels[2])\n",
    "        \n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "            # Calculate test top-1 accuracy\n",
    "            test_accuracy += (get_accuracy(grapheme_root, labels[0]) + get_accuracy(vowel_diacritic, labels[1]) + \\\n",
    "                           get_accuracy(consonant_diacritic, labels[2])) / 3.0\n",
    "            \n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Time per epoch: {time_elapsed:.4f}.. \"\n",
    "          f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n",
    "          f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n",
    "          f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n",
    "          f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n",
    "          f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \")\n",
    "\n",
    "    train_stats = train_stats.append({'Epoch': epoch, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader),\n",
    "                                      'Train accuracy': train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader),\n",
    "                                      'Test accuracy': test_accuracy/len(testloader)}, ignore_index=True)\n",
    "    \n",
    "    run.log_observation(\"time_per_epoch\", time_elapsed)\n",
    "    run.log_observation(\"time_per_step\", time_elapsed/len(trainloader))\n",
    "    run.log_observation(\"train_loss\", running_loss/len(trainloader))\n",
    "    run.log_observation(\"test_loss\", test_loss/len(testloader))\n",
    "    \n",
    "    filename = 'effnet_'+ str(epoch+1) + '.pth'\n",
    "    checkpoint = {'state_dict': model.state_dict()}\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "    running_loss = 0\n",
    "    steps = 0\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights and training statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'effnet_'+ str(epochs) + '.pth'\n",
    "\n",
    "checkpoint = {'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload complete (model.pkl)\n"
     ]
    }
   ],
   "source": [
    "run.log_dataset('model', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Time per epoch</th>\n",
       "      <th>Avg time per step</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Test loss</th>\n",
       "      <th>Test accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5550.563564</td>\n",
       "      <td>2.210499</td>\n",
       "      <td>7.743629</td>\n",
       "      <td>0.544896</td>\n",
       "      <td>3.186750</td>\n",
       "      <td>0.701234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5513.690551</td>\n",
       "      <td>2.195815</td>\n",
       "      <td>2.448289</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>2.130714</td>\n",
       "      <td>0.791517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5515.167624</td>\n",
       "      <td>2.196403</td>\n",
       "      <td>1.932893</td>\n",
       "      <td>0.808831</td>\n",
       "      <td>1.916581</td>\n",
       "      <td>0.813636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5551.349283</td>\n",
       "      <td>2.210812</td>\n",
       "      <td>1.684420</td>\n",
       "      <td>0.832485</td>\n",
       "      <td>1.806811</td>\n",
       "      <td>0.824285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5514.958261</td>\n",
       "      <td>2.196319</td>\n",
       "      <td>1.501324</td>\n",
       "      <td>0.850076</td>\n",
       "      <td>1.637139</td>\n",
       "      <td>0.840399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5954.405040</td>\n",
       "      <td>2.371328</td>\n",
       "      <td>1.353085</td>\n",
       "      <td>0.863202</td>\n",
       "      <td>1.622958</td>\n",
       "      <td>0.843617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5808.337966</td>\n",
       "      <td>2.313157</td>\n",
       "      <td>1.241957</td>\n",
       "      <td>0.873915</td>\n",
       "      <td>1.608040</td>\n",
       "      <td>0.849746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5588.338115</td>\n",
       "      <td>2.225543</td>\n",
       "      <td>1.149901</td>\n",
       "      <td>0.882772</td>\n",
       "      <td>1.623211</td>\n",
       "      <td>0.847980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5593.251053</td>\n",
       "      <td>2.227499</td>\n",
       "      <td>1.073132</td>\n",
       "      <td>0.889652</td>\n",
       "      <td>1.623732</td>\n",
       "      <td>0.852715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.0</td>\n",
       "      <td>5606.768870</td>\n",
       "      <td>2.232883</td>\n",
       "      <td>1.012609</td>\n",
       "      <td>0.896122</td>\n",
       "      <td>1.708843</td>\n",
       "      <td>0.850617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.0</td>\n",
       "      <td>5655.896184</td>\n",
       "      <td>2.252448</td>\n",
       "      <td>0.964362</td>\n",
       "      <td>0.900851</td>\n",
       "      <td>1.708042</td>\n",
       "      <td>0.852765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5625.117227</td>\n",
       "      <td>2.240190</td>\n",
       "      <td>0.917923</td>\n",
       "      <td>0.905053</td>\n",
       "      <td>1.769008</td>\n",
       "      <td>0.849348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch  Time per epoch  Avg time per step  Train loss  Train accuracy  \\\n",
       "0     0.0     5550.563564           2.210499    7.743629        0.544896   \n",
       "1     0.0     5513.690551           2.195815    2.448289        0.762329   \n",
       "2     0.0     5515.167624           2.196403    1.932893        0.808831   \n",
       "3     1.0     5551.349283           2.210812    1.684420        0.832485   \n",
       "4     2.0     5514.958261           2.196319    1.501324        0.850076   \n",
       "5     3.0     5954.405040           2.371328    1.353085        0.863202   \n",
       "6     4.0     5808.337966           2.313157    1.241957        0.873915   \n",
       "7     5.0     5588.338115           2.225543    1.149901        0.882772   \n",
       "8     6.0     5593.251053           2.227499    1.073132        0.889652   \n",
       "9     7.0     5606.768870           2.232883    1.012609        0.896122   \n",
       "10    8.0     5655.896184           2.252448    0.964362        0.900851   \n",
       "11    9.0     5625.117227           2.240190    0.917923        0.905053   \n",
       "\n",
       "    Test loss  Test accuracy  \n",
       "0    3.186750       0.701234  \n",
       "1    2.130714       0.791517  \n",
       "2    1.916581       0.813636  \n",
       "3    1.806811       0.824285  \n",
       "4    1.637139       0.840399  \n",
       "5    1.622958       0.843617  \n",
       "6    1.608040       0.849746  \n",
       "7    1.623211       0.847980  \n",
       "8    1.623732       0.852715  \n",
       "9    1.708843       0.850617  \n",
       "10   1.708042       0.852765  \n",
       "11   1.769008       0.849348  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.to_csv('train_stats_{}.csv'.format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload complete (train_stats.pkl)\n"
     ]
    }
   ],
   "source": [
    "run.log_dataset('train_stats', train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_stats['Train loss'], label='train')\n",
    "plt.plot(train_stats['Test loss'], label='test')\n",
    "plt.title('Loss over epoch')\n",
    "plt.legend()\n",
    "\n",
    "run.log_image(\"loss\", plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_stats['Train accuracy'], label='train')\n",
    "plt.plot(train_stats['Test accuracy'], label='test')\n",
    "plt.title('Accuracy over epoch')\n",
    "plt.legend()\n",
    "run.log_image(\"accuracy\", plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "model.eval()\n",
    "for img, labels in testloader:\n",
    "    img, labels = img.to(device), [label.to(device) for label in labels]\n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(img)\n",
    "    \n",
    "    img = img.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    # visualize the inputs\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(10,15))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[0].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        \n",
    "        prop = FontProperties()\n",
    "        prop.set_file('./kalpurush.ttf')\n",
    "        grapheme_root_str = class_map[(class_map.component_type == 'grapheme_root') \\\n",
    "                                  & (class_map.label == int(labels[0][i]))].component.values[0]\n",
    "        \n",
    "        vowel_diacritic_str = class_map[(class_map.component_type == 'vowel_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[1][i]))].component.values[0]\n",
    "        \n",
    "        consonant_diacritic_str = class_map[(class_map.component_type == 'consonant_diacritic') \\\n",
    "                                  & (class_map.label == int(labels[2][i]))].component.values[0]\n",
    "        \n",
    "        axs[0].set_title('{}, {}, {}'.format(grapheme_root_str, vowel_diacritic_str, consonant_diacritic_str), \n",
    "                         fontproperties=prop, fontsize=20)\n",
    "        \n",
    "        # analyze grapheme root prediction\n",
    "        ps_root = F.softmax(grapheme_root[i])\n",
    "        top10_p, top10_class = ps_root.topk(10, dim=0)\n",
    "        \n",
    "        top10_p = top10_p.detach().numpy()\n",
    "        top10_class = top10_class.detach().numpy()\n",
    "        \n",
    "        axs[1].bar(range(len(top10_p)), top10_p)\n",
    "        axs[1].set_xticks(range(len(top10_p)))\n",
    "        axs[1].set_xticklabels(top10_class)\n",
    "        axs[1].set_title('grapheme_root: {}'.format(labels[0][i]))\n",
    "        \n",
    "        # analyze vowel prediction\n",
    "        ps_vowel = F.softmax(vowel_diacritic[i])\n",
    "        top11_p, top11_class = ps_vowel.topk(11, dim=0)\n",
    "        \n",
    "        top11_p = top11_p.detach().numpy()\n",
    "        top11_class = top11_class.detach().numpy()\n",
    "        \n",
    "        axs[2].bar(range(len(top11_p)), top11_p)\n",
    "        axs[2].set_xticks(range(len(top11_p)))\n",
    "        axs[2].set_xticklabels(top11_class)\n",
    "        axs[2].set_title('vowel_diacritic: {}'.format(labels[1][i]))\n",
    "        \n",
    "        # analyze consonant prediction\n",
    "        ps_cons = F.softmax(consonant_diacritic[i])\n",
    "        top7_p, top7_class = ps_cons.topk(7, dim=0)\n",
    "        \n",
    "        top7_p = top7_p.detach().numpy()\n",
    "        top7_class = top7_class.detach().numpy()\n",
    "        \n",
    "        axs[3].bar(range(len(top7_p)), top7_p)\n",
    "        axs[3].set_xticks(range(len(top7_p)))\n",
    "        axs[3].set_xticklabels(top7_class)\n",
    "        axs[3].set_title('consonant_diacritic: {}'.format(labels[2][i]))\n",
    "        \n",
    "        plt.show()\n",
    "        break;\n",
    "        \n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize test dataset and visualize sample test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train dataset\n",
    "test_dataset = BengaliDataset(test, valid_transforms(), test_labels, validation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_validloader = DataLoader(test_dataset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample train data\n",
    "for img, image_ids in sample_validloader:\n",
    "    fig, axs = plt.subplots(1, img.shape[0], figsize=(15,10))\n",
    "    for i in range(0, img.shape[0]):\n",
    "        axs[i].imshow(TF.to_pil_image(img[i].reshape(SIZE, SIZE)), cmap='gray')\n",
    "        axs[i].set_title(image_ids[i])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_label(ps):\n",
    "    ps = F.softmax(ps)[0]\n",
    "    top_p, top_class = ps.topk(1, dim=0)\n",
    "        \n",
    "    top_p = top_p.detach().numpy()\n",
    "    top_class = top_class.detach().numpy()\n",
    "    \n",
    "    return top_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=['row_id', 'target'])\n",
    "\n",
    "for imgs, image_ids in validloader:\n",
    "    img = imgs[0]\n",
    "    image_id = image_ids[0]\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    grapheme_root, vowel_diacritic, consonant_diacritic  = model.forward(imgs)\n",
    "    \n",
    "    imgs = imgs.cpu()\n",
    "    grapheme_root = grapheme_root.cpu()\n",
    "    vowel_diacritic = vowel_diacritic.cpu()\n",
    "    consonant_diacritic = consonant_diacritic.cpu()\n",
    "    \n",
    "    grapheme_root_label = get_predicted_label(grapheme_root)\n",
    "    vowel_diacritic_label = get_predicted_label(vowel_diacritic)\n",
    "    consonant_diacritic_label = get_predicted_label(consonant_diacritic)\n",
    "    \n",
    "    submission = submission.append({'row_id':str(image_id)+'_grapheme_root', 'target':grapheme_root_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_vowel_diacritic', 'target':vowel_diacritic_label}, \n",
    "                                   ignore_index=True)\n",
    "    submission = submission.append({'row_id':str(image_id)+'_consonant_diacritic', 'target':consonant_diacritic_label}, \n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
